<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习基础 on 一只饺子</title>
        <link>http://localhost:1313/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</link>
        <description>Recent content in 机器学习基础 on 一只饺子</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>一只饺子</copyright><atom:link href="http://localhost:1313/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>对于机器学习公式的例子</title>
        <link>http://localhost:1313/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/</guid>
        <description>&lt;img src="http://localhost:1313/post/img/3.jpg" alt="Featured image of post 对于机器学习公式的例子" /&gt;&lt;h2 id=&#34;得到权重的值&#34;&gt;得到权重的值
&lt;/h2&gt;&lt;p&gt;以下是一个 &lt;strong&gt;单层神经网络（感知机）&lt;/strong&gt; 的完整示例，通过 &lt;strong&gt;手动模拟训练过程&lt;/strong&gt;，展示如何从数据中学习权重。我们以 &lt;strong&gt;房价预测&lt;/strong&gt; 为例，假设数据仅包含一个样本，目标是让模型学会调整权重和偏置。&lt;/p&gt;
&lt;h3 id=&#34;问题设定&#34;&gt;&lt;strong&gt;问题设定&lt;/strong&gt;
&lt;/h3&gt;&lt;h4 id=&#34;输入特征&#34;&gt;&lt;strong&gt;输入特征&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;$ x_1 $（面积）：1（标准化后的值，如100平方米）&lt;/li&gt;
&lt;li&gt;$ x_2 $（房龄）：1（标准化后的值，如5年）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;真实输出&#34;&gt;&lt;strong&gt;真实输出&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;$ y_{\text{true}} = 3 $（单位：万元）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;模型结构&#34;&gt;&lt;strong&gt;模型结构&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;线性模型&lt;/strong&gt;：$ y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;初始参数&lt;/strong&gt;（随机初始化）：
&lt;ul&gt;
&lt;li&gt;权重：$ w_1 = 0.5 $, $ w_2 = -0.3 $&lt;/li&gt;
&lt;li&gt;偏置：$ b = 0.2 $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;目标&#34;&gt;&lt;strong&gt;目标&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;通过梯度下降，调整 $ w_1, w_2, b $，使得 $ y_{\text{pred}} $ 接近真实值 3。&lt;/p&gt;
&lt;h3 id=&#34;训练过程&#34;&gt;&lt;strong&gt;训练过程&lt;/strong&gt;
&lt;/h3&gt;&lt;h4 id=&#34;前向传播计算预测值&#34;&gt;&lt;strong&gt;前向传播（计算预测值）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;$$
y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b = 0.5 \times 1 + (-0.3) \times 1 + 0.2 = 0.5 - 0.3 + 0.2 = 0.4
$$
此时预测值为 0.4 万元，与真实值 3 相差较大。&lt;/p&gt;
&lt;h4 id=&#34;计算损失均方误差&#34;&gt;&lt;strong&gt;计算损失（均方误差）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;$$
\text{Loss} = (y_{\text{true}} - y_{\text{pred}})^2 = (3 - 0.4)^2 = 6.76
$$&lt;/p&gt;
&lt;h4 id=&#34;反向传播计算梯度&#34;&gt;&lt;strong&gt;反向传播（计算梯度）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;对每个参数求偏导（链式法则）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;损失对 $ w_1 $ 的梯度&lt;/strong&gt;：
$$
\frac{\partial \text{Loss}}{\partial w_1} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_1 = 2(0.4 - 3) \times 1 = -5.2
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失对 $ w_2 $ 的梯度&lt;/strong&gt;：
$$
\frac{\partial \text{Loss}}{\partial w_2} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_2 = 2(0.4 - 3) \times 1 = -5.2
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失对 $ b $ 的梯度&lt;/strong&gt;：
$$
\frac{\partial \text{Loss}}{\partial b} = 2(y_{\text{pred}} - y_{\text{true}}) = 2(0.4 - 3) = -5.2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;更新参数梯度下降&#34;&gt;&lt;strong&gt;更新参数（梯度下降）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;设定学习率 $ \eta = 0.1 $，更新规则：
$$
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial \text{Loss}}{\partial w}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更新 $ w_1 $&lt;/strong&gt;：
$$
w_1 = 0.5 - 0.1 \times (-5.2) = 0.5 + 0.52 = 1.02
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新 $ w_2 $&lt;/strong&gt;：
$$
w_2 = -0.3 - 0.1 \times (-5.2) = -0.3 + 0.52 = 0.22
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新 $ b $&lt;/strong&gt;：
$$
b = 0.2 - 0.1 \times (-5.2) = 0.2 + 0.52 = 0.72
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;更新后的预测&#34;&gt;&lt;strong&gt;更新后的预测&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;使用新参数重新计算预测值：
$$
y_{\text{pred}} = 1.02 \times 1 + 0.22 \times 1 + 0.72 = 1.02 + 0.22 + 0.72 = 1.96
$$
损失更新为：
$$
\text{Loss} = (3 - 1.96)^2 = 1.08
$$
&lt;strong&gt;仅一次迭代，损失从 6.76 下降到 1.08&lt;/strong&gt;，说明权重调整有效。&lt;/p&gt;
&lt;h3 id=&#34;多轮迭代后的结果&#34;&gt;&lt;strong&gt;多轮迭代后的结果&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;重复上述过程（假设学习率不变）：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;迭代次数&lt;/th&gt;
          &lt;th&gt;$ w_1 $&lt;/th&gt;
          &lt;th&gt;$ w_2 $&lt;/th&gt;
          &lt;th&gt;$ b $&lt;/th&gt;
          &lt;th&gt;$ y_{\text{pred}} $&lt;/th&gt;
          &lt;th&gt;Loss&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0.5&lt;/td&gt;
          &lt;td&gt;-0.3&lt;/td&gt;
          &lt;td&gt;0.2&lt;/td&gt;
          &lt;td&gt;0.4&lt;/td&gt;
          &lt;td&gt;6.76&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1.02&lt;/td&gt;
          &lt;td&gt;0.22&lt;/td&gt;
          &lt;td&gt;0.72&lt;/td&gt;
          &lt;td&gt;1.96&lt;/td&gt;
          &lt;td&gt;1.08&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1.45&lt;/td&gt;
          &lt;td&gt;0.65&lt;/td&gt;
          &lt;td&gt;1.17&lt;/td&gt;
          &lt;td&gt;2.60&lt;/td&gt;
          &lt;td&gt;0.16&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;1.68&lt;/td&gt;
          &lt;td&gt;0.89&lt;/td&gt;
          &lt;td&gt;1.43&lt;/td&gt;
          &lt;td&gt;2.96&lt;/td&gt;
          &lt;td&gt;0.0016&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;经过3次迭代，预测值 $ y_{\text{pred}} = 2.96 $ 接近真实值3，损失降至0.0016。&lt;/p&gt;
&lt;h3 id=&#34;关键结论&#34;&gt;&lt;strong&gt;关键结论&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;权重的本质&lt;/strong&gt;：模型通过梯度下降，沿着损失减小的方向调整权重，逐步逼近真实值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习率的作用&lt;/strong&gt;：学习率 $ \eta $ 控制参数更新步幅（过大可能导致震荡，过小收敛慢）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实际训练&lt;/strong&gt;：真实场景中需使用大量数据分批训练，而非单个样本。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;附：Python代码模拟&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 初始参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 前向传播&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 计算梯度&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dL_dw1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dL_dw2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dL_db&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 更新参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dL_dw1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dL_dw2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dL_db&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Epoch &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;: w1=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, w2=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, b=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, y_pred=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, Loss=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.4f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;y--b--sum_i-c_i--textsigmoidb_i--sum_j-w_ij-x_j&#34;&gt;$y = b + \sum_i c_i , \text{sigmoid}(b_i + \sum_j w_{ij} x_j)$
&lt;/h2&gt;&lt;p&gt;假设我们要根据房屋的两个特征预测房价&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征1($x_1$)&lt;/strong&gt;：面积（平方米）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征2($x_2$)&lt;/strong&gt;：房龄（年）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们设计一个简单的神经网络，结构如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入层：&lt;/strong&gt; 两个特征（$x_1,x_2$）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;： 2个神经元（$i=1,2$）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出层&lt;/strong&gt;： 1个输出（房价$y$）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-1设定参数值&#34;&gt;step 1：设定参数值
&lt;/h3&gt;&lt;p&gt;假设模型已经训练完成，参数如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐藏层参数&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;神经元&lt;/th&gt;
          &lt;th&gt;权重$w_{i1}$（面积权重）&lt;/th&gt;
          &lt;th&gt;权重$w_{i2}$(房龄权重)&lt;/th&gt;
          &lt;th&gt;偏置$b_i$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1$(i=1)$&lt;/td&gt;
          &lt;td&gt;0.8&lt;/td&gt;
          &lt;td&gt;-0.2&lt;/td&gt;
          &lt;td&gt;0.5&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2$(i=2)$&lt;/td&gt;
          &lt;td&gt;0.5&lt;/td&gt;
          &lt;td&gt;-0.6&lt;/td&gt;
          &lt;td&gt;-0.3&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;输出层参数&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;权重$c_i$&lt;/th&gt;
          &lt;th&gt;偏置$b$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;$c_1=10$&lt;/td&gt;
          &lt;td&gt;$b=5$&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$c_2=-8$&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;step-2输出数据&#34;&gt;step 2：输出数据
&lt;/h3&gt;&lt;p&gt;假设有一套房子的特征值为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;面积$x_1=100m^2$&lt;/li&gt;
&lt;li&gt;房龄$x_2=5年$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-3计算隐藏层输出&#34;&gt;step 3：计算隐藏层输出
&lt;/h3&gt;&lt;p&gt;对每个隐藏层神经元，计算$z_i,=,b_i,+,w_{i1}x_1,+,w_{i2}x_2$，然后通过$sigmoid$激活函数得到$a_i=sigmoid(z_i)$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;神经元1($i=1$)的计算&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z_1 = b_1 + w_{11}x_1 + w_{12}x_2 = 0.5 + 0.8 \times 100 + (-0.2) \times 5 = 0.5 + 80 - 1 = 79.5\
a_1 = \text{sigmoid}(79.5) = \frac{1}{1 + e^{-79.5}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;神经元2($i=2$)的计算&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z_2 = b_2 + w_{21}x_1 + w_{22}x_2 = -0.3 + 0.5 \times 100 + (-0.6) \times 5 = -0.3 + 50 - 3 = 46.7\
a_2 = \text{sigmoid}(46.7) = \frac{1}{1 + e^{-46.7}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;step-4计算输出层结果&#34;&gt;step 4：计算输出层结果
&lt;/h3&gt;&lt;p&gt;$$
y = b + c_1 a_1 + c_2 a_2 = 5 + 10 \times 1.0 + (-8) \times 1.0 = 5 + 10 - 8 = 7
$$&lt;/p&gt;
&lt;h2 id=&#34;lthetaapprox-ltheta-ltheta---thetagfrac12theta-thetaththeta-theta-&#34;&gt;$L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})$
&lt;/h2&gt;&lt;p&gt;由于&lt;del&gt;线性代数学艺不精&lt;/del&gt;热爱线性代数，重新推导这个公式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;回忆一维泰勒展开&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如，在$x&amp;rsquo;$附件展开$f(x)$
$$
f(x)\approx f(x&amp;rsquo;)+f&amp;rsquo;(x&amp;rsquo;)(x-x&amp;rsquo;)+\frac{1}{2}f&amp;quot;(x&amp;rsquo;)(x-x&amp;rsquo;)^2
$$&lt;/p&gt;
&lt;p&gt;对于泰勒展开公式：
$$
f(x_0,x)=\sum_{i=0}^n \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i
$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;扩展到多维情况（参数$\theta$是向量）&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})$&lt;/p&gt;
&lt;p&gt;在多维情况下，参数是向量$\theta = [\theta_1,\theta_2,&amp;hellip;\theta_n]^T$，梯度$g$是一阶导数的推广&lt;/p&gt;
&lt;p&gt;对于&lt;strong&gt;Hessian&lt;/strong&gt;矩阵，是多元函数的二阶偏导数构成的矩阵
$$
H = \nabla^2 f = \begin{bmatrix}
\frac{\partial^2 f}{\partial \theta_1^2} &amp;amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_n} \
\frac{\partial^2 f}{\partial \theta_2 \partial \theta_1} &amp;amp; \frac{\partial^2 f}{\partial \theta_2^2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_2 \partial \theta_n} \
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \
\frac{\partial^2 f}{\partial \theta_n \partial \theta_1} &amp;amp; \frac{\partial^2 f}{\partial \theta_n \partial \theta_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_n^2}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么需要转置 $(\theta - \theta&amp;rsquo;)^\top$？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;维度匹配&lt;/strong&gt;：假设 $\theta$ 是 $n \times 1$ 向量，梯度 $g$ 也是 $n \times 1$，Hessian H$ $H$是 $n \times n$。
&lt;ul&gt;
&lt;li&gt;一阶项：$(\theta - \theta &amp;lsquo;)^Tg$是$n \times 1$向量，梯度$g$也是$n\times 1$，Hessian $H$是$n \times n$（标量）
&lt;ul&gt;
&lt;li&gt;一阶项：$(\theta - \theta &amp;lsquo;)g$是$1\times n*n\times n * n \times 1=1\times 1$（标量）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数学必要性&lt;/strong&gt;：转置确保矩阵乘法维度相容。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;一个具体的例子&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;1-定义函数&#34;&gt;&lt;strong&gt;1. 定义函数&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;设损失函数 $L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2$，参考点 $\theta&amp;rsquo; = [0, 0]^\top$。&lt;/p&gt;
&lt;h5 id=&#34;2-计算梯度-g&#34;&gt;&lt;strong&gt;2. 计算梯度 $g$&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;$$
g = \nabla L(\theta&amp;rsquo;) = \begin{bmatrix} 2\theta_1 + \theta_2 \ 4\theta_2 + \theta_1 \end{bmatrix} \bigg|_{\theta&amp;rsquo;=[0,0]} = \begin{bmatrix} 0 \ 0 \end{bmatrix}
$$&lt;/p&gt;
&lt;h5 id=&#34;3-计算-hessian-矩阵-h&#34;&gt;&lt;strong&gt;3. 计算 Hessian 矩阵 $H$&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;$$
H = \nabla^2 L(\theta&amp;rsquo;) = \begin{bmatrix}
\frac{\partial^2 L}{\partial \theta_1^2} &amp;amp; \frac{\partial^2 L}{\partial \theta_1 \partial \theta_2} \
\frac{\partial^2 L}{\partial \theta_2 \partial \theta_1} &amp;amp; \frac{\partial^2 L}{\partial \theta_2^2}
\end{bmatrix} = \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix}
$$&lt;/p&gt;
&lt;h5 id=&#34;4-泰勒展开公式&#34;&gt;&lt;strong&gt;4. 泰勒展开公式&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;在 $\theta&amp;rsquo; = [0, 0]^\top$ 处展开：
$$
L(\theta) \approx \underbrace{0}&lt;em&gt;{L(\theta&amp;rsquo;)} + \underbrace{(\theta - 0)^\top \begin{bmatrix} 0 \ 0 \end{bmatrix}}&lt;/em&gt;{\text{一阶项}} + \frac{1}{2}(\theta - 0)^\top \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix} (\theta - 0)
$$&lt;/p&gt;
&lt;p&gt;化简后：
$$
L(\theta) \approx \frac{1}{2}\theta^\top \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix} \theta = \frac{1}{2}(2\theta_1^2 + 2\theta_1\theta_2 + 4\theta_2^2)
$$&lt;/p&gt;
&lt;p&gt;展开后与原函数一致：
$$
L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2
$$&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
