<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>线性代数 on 一只饺子</title>
        <link>http://localhost:1313/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</link>
        <description>Recent content in 线性代数 on 一只饺子</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>一只饺子</copyright><atom:link href="http://localhost:1313/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>对于机器学习公式的例子</title>
        <link>http://localhost:1313/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/</guid>
        <description>&lt;img src="http://localhost:1313/post/img/3.jpg" alt="Featured image of post 对于机器学习公式的例子" /&gt;&lt;h2 id=&#34;得到权重的值&#34;&gt;得到权重的值
&lt;/h2&gt;&lt;p&gt;以下是一个 &lt;strong&gt;单层神经网络（感知机）&lt;/strong&gt; 的完整示例，通过 &lt;strong&gt;手动模拟训练过程&lt;/strong&gt;，展示如何从数据中学习权重。我们以 &lt;strong&gt;房价预测&lt;/strong&gt; 为例，假设数据仅包含一个样本，目标是让模型学会调整权重和偏置。&lt;/p&gt;
&lt;h3 id=&#34;问题设定&#34;&gt;&lt;strong&gt;问题设定&lt;/strong&gt;
&lt;/h3&gt;&lt;h4 id=&#34;输入特征&#34;&gt;&lt;strong&gt;输入特征&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;$ x_1 $（面积）：1（标准化后的值，如100平方米）&lt;/li&gt;
&lt;li&gt;$ x_2 $（房龄）：1（标准化后的值，如5年）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;真实输出&#34;&gt;&lt;strong&gt;真实输出&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;$ y_{\text{true}} = 3 $（单位：万元）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;模型结构&#34;&gt;&lt;strong&gt;模型结构&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;线性模型&lt;/strong&gt;：$ y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b $&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;初始参数&lt;/strong&gt;（随机初始化）：
&lt;ul&gt;
&lt;li&gt;权重：$ w_1 = 0.5 $, $ w_2 = -0.3 $&lt;/li&gt;
&lt;li&gt;偏置：$ b = 0.2 $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;目标&#34;&gt;&lt;strong&gt;目标&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;通过梯度下降，调整 $ w_1, w_2, b $，使得 $ y_{\text{pred}} $ 接近真实值 3。&lt;/p&gt;
&lt;h3 id=&#34;训练过程&#34;&gt;&lt;strong&gt;训练过程&lt;/strong&gt;
&lt;/h3&gt;&lt;h4 id=&#34;前向传播计算预测值&#34;&gt;&lt;strong&gt;前向传播（计算预测值）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;$$
y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b = 0.5 \times 1 + (-0.3) \times 1 + 0.2 = 0.5 - 0.3 + 0.2 = 0.4
$$
此时预测值为 0.4 万元，与真实值 3 相差较大。&lt;/p&gt;
&lt;h4 id=&#34;计算损失均方误差&#34;&gt;&lt;strong&gt;计算损失（均方误差）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;$$
\text{Loss} = (y_{\text{true}} - y_{\text{pred}})^2 = (3 - 0.4)^2 = 6.76
$$&lt;/p&gt;
&lt;h4 id=&#34;反向传播计算梯度&#34;&gt;&lt;strong&gt;反向传播（计算梯度）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;对每个参数求偏导（链式法则）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;损失对 $ w_1 $ 的梯度&lt;/strong&gt;：
$$
\frac{\partial \text{Loss}}{\partial w_1} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_1 = 2(0.4 - 3) \times 1 = -5.2
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失对 $ w_2 $ 的梯度&lt;/strong&gt;：
$$
\frac{\partial \text{Loss}}{\partial w_2} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_2 = 2(0.4 - 3) \times 1 = -5.2
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失对 $ b $ 的梯度&lt;/strong&gt;：
$$
\frac{\partial \text{Loss}}{\partial b} = 2(y_{\text{pred}} - y_{\text{true}}) = 2(0.4 - 3) = -5.2
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;更新参数梯度下降&#34;&gt;&lt;strong&gt;更新参数（梯度下降）&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;设定学习率 $ \eta = 0.1 $，更新规则：
$$
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial \text{Loss}}{\partial w}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;更新 $ w_1 $&lt;/strong&gt;：
$$
w_1 = 0.5 - 0.1 \times (-5.2) = 0.5 + 0.52 = 1.02
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新 $ w_2 $&lt;/strong&gt;：
$$
w_2 = -0.3 - 0.1 \times (-5.2) = -0.3 + 0.52 = 0.22
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新 $ b $&lt;/strong&gt;：
$$
b = 0.2 - 0.1 \times (-5.2) = 0.2 + 0.52 = 0.72
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;更新后的预测&#34;&gt;&lt;strong&gt;更新后的预测&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;使用新参数重新计算预测值：
$$
y_{\text{pred}} = 1.02 \times 1 + 0.22 \times 1 + 0.72 = 1.02 + 0.22 + 0.72 = 1.96
$$
损失更新为：
$$
\text{Loss} = (3 - 1.96)^2 = 1.08
$$
&lt;strong&gt;仅一次迭代，损失从 6.76 下降到 1.08&lt;/strong&gt;，说明权重调整有效。&lt;/p&gt;
&lt;h3 id=&#34;多轮迭代后的结果&#34;&gt;&lt;strong&gt;多轮迭代后的结果&lt;/strong&gt;
&lt;/h3&gt;&lt;p&gt;重复上述过程（假设学习率不变）：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;迭代次数&lt;/th&gt;
          &lt;th&gt;$ w_1 $&lt;/th&gt;
          &lt;th&gt;$ w_2 $&lt;/th&gt;
          &lt;th&gt;$ b $&lt;/th&gt;
          &lt;th&gt;$ y_{\text{pred}} $&lt;/th&gt;
          &lt;th&gt;Loss&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0.5&lt;/td&gt;
          &lt;td&gt;-0.3&lt;/td&gt;
          &lt;td&gt;0.2&lt;/td&gt;
          &lt;td&gt;0.4&lt;/td&gt;
          &lt;td&gt;6.76&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;1.02&lt;/td&gt;
          &lt;td&gt;0.22&lt;/td&gt;
          &lt;td&gt;0.72&lt;/td&gt;
          &lt;td&gt;1.96&lt;/td&gt;
          &lt;td&gt;1.08&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;1.45&lt;/td&gt;
          &lt;td&gt;0.65&lt;/td&gt;
          &lt;td&gt;1.17&lt;/td&gt;
          &lt;td&gt;2.60&lt;/td&gt;
          &lt;td&gt;0.16&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3&lt;/td&gt;
          &lt;td&gt;1.68&lt;/td&gt;
          &lt;td&gt;0.89&lt;/td&gt;
          &lt;td&gt;1.43&lt;/td&gt;
          &lt;td&gt;2.96&lt;/td&gt;
          &lt;td&gt;0.0016&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;经过3次迭代，预测值 $ y_{\text{pred}} = 2.96 $ 接近真实值3，损失降至0.0016。&lt;/p&gt;
&lt;h3 id=&#34;关键结论&#34;&gt;&lt;strong&gt;关键结论&lt;/strong&gt;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;权重的本质&lt;/strong&gt;：模型通过梯度下降，沿着损失减小的方向调整权重，逐步逼近真实值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习率的作用&lt;/strong&gt;：学习率 $ \eta $ 控制参数更新步幅（过大可能导致震荡，过小收敛慢）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实际训练&lt;/strong&gt;：真实场景中需使用大量数据分批训练，而非单个样本。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;附：Python代码模拟&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 初始参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 前向传播&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 计算梯度&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dL_dw1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dL_dw2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dL_db&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 更新参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dL_dw1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dL_dw2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dL_db&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Epoch &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;epoch&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;: w1=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, w2=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, b=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, y_pred=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.2f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;, Loss=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.4f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;y--b--sum_i-c_i--textsigmoidb_i--sum_j-w_ij-x_j&#34;&gt;$y = b + \sum_i c_i , \text{sigmoid}(b_i + \sum_j w_{ij} x_j)$
&lt;/h2&gt;&lt;p&gt;假设我们要根据房屋的两个特征预测房价&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征1($x_1$)&lt;/strong&gt;：面积（平方米）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征2($x_2$)&lt;/strong&gt;：房龄（年）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们设计一个简单的神经网络，结构如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入层：&lt;/strong&gt; 两个特征（$x_1,x_2$）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;： 2个神经元（$i=1,2$）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出层&lt;/strong&gt;： 1个输出（房价$y$）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-1设定参数值&#34;&gt;step 1：设定参数值
&lt;/h3&gt;&lt;p&gt;假设模型已经训练完成，参数如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;隐藏层参数&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;神经元&lt;/th&gt;
          &lt;th&gt;权重$w_{i1}$（面积权重）&lt;/th&gt;
          &lt;th&gt;权重$w_{i2}$(房龄权重)&lt;/th&gt;
          &lt;th&gt;偏置$b_i$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1$(i=1)$&lt;/td&gt;
          &lt;td&gt;0.8&lt;/td&gt;
          &lt;td&gt;-0.2&lt;/td&gt;
          &lt;td&gt;0.5&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2$(i=2)$&lt;/td&gt;
          &lt;td&gt;0.5&lt;/td&gt;
          &lt;td&gt;-0.6&lt;/td&gt;
          &lt;td&gt;-0.3&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;输出层参数&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;权重$c_i$&lt;/th&gt;
          &lt;th&gt;偏置$b$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;$c_1=10$&lt;/td&gt;
          &lt;td&gt;$b=5$&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$c_2=-8$&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;step-2输出数据&#34;&gt;step 2：输出数据
&lt;/h3&gt;&lt;p&gt;假设有一套房子的特征值为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;面积$x_1=100m^2$&lt;/li&gt;
&lt;li&gt;房龄$x_2=5年$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-3计算隐藏层输出&#34;&gt;step 3：计算隐藏层输出
&lt;/h3&gt;&lt;p&gt;对每个隐藏层神经元，计算$z_i,=,b_i,+,w_{i1}x_1,+,w_{i2}x_2$，然后通过$sigmoid$激活函数得到$a_i=sigmoid(z_i)$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;神经元1($i=1$)的计算&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z_1 = b_1 + w_{11}x_1 + w_{12}x_2 = 0.5 + 0.8 \times 100 + (-0.2) \times 5 = 0.5 + 80 - 1 = 79.5\
a_1 = \text{sigmoid}(79.5) = \frac{1}{1 + e^{-79.5}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;神经元2($i=2$)的计算&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
z_2 = b_2 + w_{21}x_1 + w_{22}x_2 = -0.3 + 0.5 \times 100 + (-0.6) \times 5 = -0.3 + 50 - 3 = 46.7\
a_2 = \text{sigmoid}(46.7) = \frac{1}{1 + e^{-46.7}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$&lt;/p&gt;
&lt;h3 id=&#34;step-4计算输出层结果&#34;&gt;step 4：计算输出层结果
&lt;/h3&gt;&lt;p&gt;$$
y = b + c_1 a_1 + c_2 a_2 = 5 + 10 \times 1.0 + (-8) \times 1.0 = 5 + 10 - 8 = 7
$$&lt;/p&gt;
&lt;h2 id=&#34;lthetaapprox-ltheta-ltheta---thetagfrac12theta-thetaththeta-theta-&#34;&gt;$L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})$
&lt;/h2&gt;&lt;p&gt;由于&lt;del&gt;线性代数学艺不精&lt;/del&gt;热爱线性代数，重新推导这个公式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;回忆一维泰勒展开&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如，在$x&amp;rsquo;$附件展开$f(x)$
$$
f(x)\approx f(x&amp;rsquo;)+f&amp;rsquo;(x&amp;rsquo;)(x-x&amp;rsquo;)+\frac{1}{2}f&amp;quot;(x&amp;rsquo;)(x-x&amp;rsquo;)^2
$$&lt;/p&gt;
&lt;p&gt;对于泰勒展开公式：
$$
f(x_0,x)=\sum_{i=0}^n \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i
$$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;扩展到多维情况（参数$\theta$是向量）&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})$&lt;/p&gt;
&lt;p&gt;在多维情况下，参数是向量$\theta = [\theta_1,\theta_2,&amp;hellip;\theta_n]^T$，梯度$g$是一阶导数的推广&lt;/p&gt;
&lt;p&gt;对于&lt;strong&gt;Hessian&lt;/strong&gt;矩阵，是多元函数的二阶偏导数构成的矩阵
$$
H = \nabla^2 f = \begin{bmatrix}
\frac{\partial^2 f}{\partial \theta_1^2} &amp;amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_n} \
\frac{\partial^2 f}{\partial \theta_2 \partial \theta_1} &amp;amp; \frac{\partial^2 f}{\partial \theta_2^2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_2 \partial \theta_n} \
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \
\frac{\partial^2 f}{\partial \theta_n \partial \theta_1} &amp;amp; \frac{\partial^2 f}{\partial \theta_n \partial \theta_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_n^2}
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为什么需要转置 $(\theta - \theta&amp;rsquo;)^\top$？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;维度匹配&lt;/strong&gt;：假设 $\theta$ 是 $n \times 1$ 向量，梯度 $g$ 也是 $n \times 1$，Hessian H$ $H$是 $n \times n$。
&lt;ul&gt;
&lt;li&gt;一阶项：$(\theta - \theta &amp;lsquo;)^Tg$是$n \times 1$向量，梯度$g$也是$n\times 1$，Hessian $H$是$n \times n$（标量）
&lt;ul&gt;
&lt;li&gt;一阶项：$(\theta - \theta &amp;lsquo;)g$是$1\times n*n\times n * n \times 1=1\times 1$（标量）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数学必要性&lt;/strong&gt;：转置确保矩阵乘法维度相容。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;一个具体的例子&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;1-定义函数&#34;&gt;&lt;strong&gt;1. 定义函数&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;设损失函数 $L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2$，参考点 $\theta&amp;rsquo; = [0, 0]^\top$。&lt;/p&gt;
&lt;h5 id=&#34;2-计算梯度-g&#34;&gt;&lt;strong&gt;2. 计算梯度 $g$&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;$$
g = \nabla L(\theta&amp;rsquo;) = \begin{bmatrix} 2\theta_1 + \theta_2 \ 4\theta_2 + \theta_1 \end{bmatrix} \bigg|_{\theta&amp;rsquo;=[0,0]} = \begin{bmatrix} 0 \ 0 \end{bmatrix}
$$&lt;/p&gt;
&lt;h5 id=&#34;3-计算-hessian-矩阵-h&#34;&gt;&lt;strong&gt;3. 计算 Hessian 矩阵 $H$&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;$$
H = \nabla^2 L(\theta&amp;rsquo;) = \begin{bmatrix}
\frac{\partial^2 L}{\partial \theta_1^2} &amp;amp; \frac{\partial^2 L}{\partial \theta_1 \partial \theta_2} \
\frac{\partial^2 L}{\partial \theta_2 \partial \theta_1} &amp;amp; \frac{\partial^2 L}{\partial \theta_2^2}
\end{bmatrix} = \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix}
$$&lt;/p&gt;
&lt;h5 id=&#34;4-泰勒展开公式&#34;&gt;&lt;strong&gt;4. 泰勒展开公式&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;在 $\theta&amp;rsquo; = [0, 0]^\top$ 处展开：
$$
L(\theta) \approx \underbrace{0}&lt;em&gt;{L(\theta&amp;rsquo;)} + \underbrace{(\theta - 0)^\top \begin{bmatrix} 0 \ 0 \end{bmatrix}}&lt;/em&gt;{\text{一阶项}} + \frac{1}{2}(\theta - 0)^\top \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix} (\theta - 0)
$$&lt;/p&gt;
&lt;p&gt;化简后：
$$
L(\theta) \approx \frac{1}{2}\theta^\top \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix} \theta = \frac{1}{2}(2\theta_1^2 + 2\theta_1\theta_2 + 4\theta_2^2)
$$&lt;/p&gt;
&lt;p&gt;展开后与原函数一致：
$$
L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2
$$&lt;/p&gt;
</description>
        </item>
        <item>
        <title>机器学习（李宏毅）笔记 4：局部最小值与鞍点</title>
        <link>http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-4%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC%E4%B8%8E%E9%9E%8D%E7%82%B9/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-4%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC%E4%B8%8E%E9%9E%8D%E7%82%B9/</guid>
        <description>&lt;img src="http://localhost:1313/post/img/7.jpg" alt="Featured image of post 机器学习（李宏毅）笔记 4：局部最小值与鞍点" /&gt;&lt;h2 id=&#34;critical-point情况&#34;&gt;Critical Point情况
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;局部最小值（local minima）&lt;/strong&gt;。如果是**卡在local minima,那可能就没有路可以走了，**因为四周都比较高，你现在所在的位置已经是最低的点，loss最低的点了，往四周走 loss都会比较高，你会不知道怎么走到其他地方去。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;鞍点（saddle point）&lt;/strong&gt;。（如图可看出，左右是比红点高，前后比红点低，红点既不是local minima,也不是local maxima的地方）如果是卡在saddle point，saddle point旁边还是有其他路可以让你的loss更低的，你只要逃离saddle point，你就有可能让你的loss更低。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/%e5%b1%80%e9%83%a8%e6%9c%80%e5%b0%8f%e5%80%bc.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;局部最小值&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/img/%e9%9e%8d%e7%82%b9.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;鞍点&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;确定critical-point类型&#34;&gt;确定Critical Point类型
&lt;/h2&gt;&lt;p&gt;使用泰勒级数近似
$$
L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})
$$
&lt;strong&gt;计算Hessian矩阵的特征值&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;正定&lt;/strong&gt;（所有特征值 &amp;gt; 0）→ &lt;strong&gt;局部极小值&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负定&lt;/strong&gt;（所有特征值 &amp;lt; 0）→ &lt;strong&gt;局部极大值&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不定&lt;/strong&gt;（特征值有正有负）→ &lt;strong&gt;鞍点&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;半正定/半负定&lt;/strong&gt;（存在零特征值）→ &lt;strong&gt;需更高阶分析&lt;/strong&gt;（如退化临界点）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;具体例子&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;案例1：正定Hessian → 局部极小值&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
H=\begin{bmatrix}
2 &amp;amp; 1 \
1 &amp;amp; 2
\end{bmatrix}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征值&lt;/strong&gt;：3和1（均 &amp;gt; 0）→ 正定&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结论&lt;/strong&gt;：局部极小值&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;案例2：负定Hessian → 局部极大值&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
H=\begin{bmatrix}
-2 &amp;amp; 0 \
0 &amp;amp; -2
\end{bmatrix}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征值&lt;/strong&gt;：-2 和 -2（均 &amp;lt; 0）→ 负定&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结论&lt;/strong&gt;：局部极大值。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;案例3：不定Hessian → 鞍点&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
H = \begin{bmatrix}
2 &amp;amp; 0 \
0 &amp;amp; -2
\end{bmatrix}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特征值&lt;/strong&gt;：2 和 -2（有正有负）→ 不定&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结论&lt;/strong&gt;：鞍点&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;逃离saddle-point&#34;&gt;逃离Saddle Point
&lt;/h2&gt;&lt;h3 id=&#34;利用hessian矩阵逃离鞍点saddle-point&#34;&gt;利用Hessian矩阵逃离鞍点（Saddle Point）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：通过Hessian矩阵的负特征值对应的特征向量方向更新参数，使优化方向逃离鞍点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;具体步骤&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1. 检测鞍点&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;计算梯度$\nabla f$，若$||\nabla f|| \approx 0$，则可能为临界点&lt;/li&gt;
&lt;li&gt;计算Hessian矩阵$H$，并分析其特征值&lt;/li&gt;
&lt;li&gt;若存在负特征值，则为鞍点。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2. 找到负曲率方向&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;对Hessian矩阵进行特征分解，找到最小特征值$\lambda_{\min} &amp;lt; 0$及其对应的特征向量$v_{\min}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3. 沿负曲率方向更新参数&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;选择步长$\eta$（通常与学习率相关），沿$v_{min}$方向更新参数：$\theta_{new}=\theta_{old}+\eta v_{min}$&lt;/li&gt;
&lt;li&gt;**验证方向：**通过计算$\theta_{new}=\theta_{old}+\eta v_{min}$和$\theta_{new}=\theta_{old}-\eta v_{min}$，选择使函数值下降的方向&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4. 迭代直至逃离&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;重复步骤1-3，直到梯度不再接近0或Hessian矩阵变为正定（局部最小值）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;利用momentum逃离鞍点&#34;&gt;利用momentum逃离鞍点
&lt;/h3&gt;&lt;h4 id=&#34;动量法的基本原理&#34;&gt;&lt;strong&gt;动量法的基本原理&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;动量法（Momentum）是梯度下降的改进版本，通过&lt;strong&gt;积累历史梯度方向&lt;/strong&gt;加速收敛并抑制振荡。其更新公式为：
$$
v_t = \beta v_{t-1} + (1-\beta) \nabla f(\theta_t)
$$&lt;/p&gt;
&lt;p&gt;$$
\theta{t+1} = \theta_t - \eta v_t
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;动量系数&lt;/strong&gt;：$$\beta \in [0, 1)$$，通常取0.9或0.99。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心思想&lt;/strong&gt;：梯度方向被赋予“惯性”，在平坦区域（如鞍点）积累动量，帮助逃离。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;动量如何帮助逃离鞍点&#34;&gt;&lt;strong&gt;动量如何帮助逃离鞍点？&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;鞍点的特征：梯度$$\nabla f \approx 0$$，但Hessian矩阵存在&lt;strong&gt;负曲率方向&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;梯度下降的缺陷&lt;/strong&gt;：在鞍点附近，梯度接近零，参数更新停滞。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动量的优势&lt;/strong&gt;：
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;历史梯度累积&lt;/strong&gt;：即使当前梯度为零，动量项$$v_t$$仍可能保留之前方向的惯性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;噪声放大&lt;/strong&gt;：随机梯度（如SGD的小批量噪声）会被动量放大，打破对称性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;负曲率方向探索&lt;/strong&gt;：动量推动参数沿历史梯度方向移动，可能进入负曲率区域。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;动量逃离鞍点的数学解释&#34;&gt;&lt;strong&gt;动量逃离鞍点的数学解释&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;假设在鞍点附近，梯度方向存在随机扰动$$\epsilon_t$$（如小批量噪声）：
$$
\nabla f(\theta_t) = \epsilon_t \quad (\mathbb{E}[\epsilon_t] = 0, \text{Var}(\epsilon_t) = \sigma^2)
$$
动量更新公式变为：
$$
v_t = \beta v_{t-1} + (1-\beta) \epsilon_t
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;动量积累&lt;/strong&gt;：经过$$k$$步后，动量近似为：
$$
v_t \approx (1-\beta) \sum_{i=0}^{k} \beta^{k-i} \epsilon_i
$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;逃离机制&lt;/strong&gt;：噪声的加权和可能指向负曲率方向，使参数突破鞍点。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;具体步骤与算法&#34;&gt;&lt;strong&gt;具体步骤与算法&lt;/strong&gt;
&lt;/h4&gt;&lt;h5 id=&#34;步骤1初始化动量&#34;&gt;&lt;strong&gt;步骤1：初始化动量&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;设置初始动量$$v_0 = 0$$，选择动量系数$\beta$和学习率$\eta$。&lt;/p&gt;
&lt;h5 id=&#34;步骤2迭代更新&#34;&gt;&lt;strong&gt;步骤2：迭代更新&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;对每次迭代$t$：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算当前梯度$\nabla f(\theta_t)$（可含噪声，如SGD）。&lt;/li&gt;
&lt;li&gt;更新动量：
$$
v_t = \beta v_{t-1} + (1-\beta) \nabla f(\theta_t)
$$&lt;/li&gt;
&lt;li&gt;更新参数：
$$
\theta_{t+1} = \theta_t - \eta v_t
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;步骤3逃离鞍点的动态&#34;&gt;&lt;strong&gt;步骤3：逃离鞍点的动态&lt;/strong&gt;
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;鞍点附近&lt;/strong&gt;：梯度$\nabla f \approx 0$，但动量$v_t$可能因历史梯度或噪声不为零。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续更新&lt;/strong&gt;：动量推动参数离开平坦区域，进入梯度较大的区域。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;实验案例动量法逃离二元鞍点&#34;&gt;&lt;strong&gt;实验案例：动量法逃离二元鞍点&lt;/strong&gt;
&lt;/h4&gt;&lt;h5 id=&#34;目标函数&#34;&gt;&lt;strong&gt;目标函数&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;$$
f(x, y) = x^2 - y^2
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;鞍点&lt;/strong&gt;：$(0, 0)$，Hessian矩阵特征值为$2$和$-2$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;参数设置&#34;&gt;&lt;strong&gt;参数设置&lt;/strong&gt;
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;初始点：$(0.1, 0.1)$，学习率$\eta = 0.1$，动量系数$\beta = 0.9$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;迭代过程&#34;&gt;&lt;strong&gt;迭代过程&lt;/strong&gt;
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;第1步&lt;/strong&gt;：梯度$\nabla f = (0.2, -0.2)$，动量$v_1 = 0.1 \times (0.2, -0.2)$，更新后点$(0.08, 0.12)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第2步&lt;/strong&gt;：梯度$\nabla f = (0.16, -0.24)$，动量$v_2 = 0.9v_1 + 0.1 \times (0.16, -0.24)$，更新后点$(0.064, 0.144)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;持续迭代&lt;/strong&gt;：动量在$y$方向逐渐积累，推动逃离鞍点区域。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;动量法的理论支持&#34;&gt;&lt;strong&gt;动量法的理论支持&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;收敛性证明&lt;/strong&gt;：在凸函数中，动量法可加速收敛（Nesterov加速）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;逃离鞍点能力&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;随机梯度（SGD）&lt;/strong&gt;：噪声+动量可概率性逃离鞍点（Ge et al., 2015）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;确定性梯度&lt;/strong&gt;：动量法需依赖Hessian的负曲率方向隐含在历史梯度中。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;与其他方法的对比&#34;&gt;&lt;strong&gt;与其他方法的对比&lt;/strong&gt;
&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;strong&gt;方法&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;逃离鞍点机制&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;计算成本&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;适用场景&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;动量法&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;历史梯度惯性 + 噪声放大&lt;/td&gt;
          &lt;td&gt;低（一阶）&lt;/td&gt;
          &lt;td&gt;高维、随机优化（如深度学习）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Hessian矩阵法&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;显式利用负曲率方向&lt;/td&gt;
          &lt;td&gt;高（二阶）&lt;/td&gt;
          &lt;td&gt;低维、确定性优化&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;SGD + 扰动&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;纯随机噪声探索&lt;/td&gt;
          &lt;td&gt;低（一阶）&lt;/td&gt;
          &lt;td&gt;大规模非凸优化&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;实际应用技巧&#34;&gt;&lt;strong&gt;实际应用技巧&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;动量系数选择&lt;/strong&gt;：$\beta$越大，惯性越强，但可能“冲过头”。常用$\beta=0.9$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;与自适应方法结合&lt;/strong&gt;：如Adam（动量+RMSProp），平衡方向与步长。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习率调整&lt;/strong&gt;：在鞍点附近可短暂增大学习率以加速逃离。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;优缺点分析&#34;&gt;&lt;strong&gt;优缺点分析&lt;/strong&gt;
&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;低计算成本，适合高维问题。&lt;/td&gt;
          &lt;td&gt;无显式二阶信息，依赖噪声或历史梯度。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;天然抗噪声，适合随机优化。&lt;/td&gt;
          &lt;td&gt;对某些鞍点（如高阶退化点）可能失效。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;易与其他优化器结合（如Adam）。&lt;/td&gt;
          &lt;td&gt;需调参（$\beta$, $\eta$）。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        
    </channel>
</rss>
