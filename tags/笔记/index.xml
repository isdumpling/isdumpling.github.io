<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>笔记 on 一只饺子</title><link>https://example.com/tags/%E7%AC%94%E8%AE%B0/</link><description>Recent content in 笔记 on 一只饺子</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>一只饺子</copyright><atom:link href="https://example.com/tags/%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>Hadoop Chapter 1: Big Data Concept(CN)</title><link>https://example.com/p/hadoop-chapter-1-big-data-conceptcn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/hadoop-chapter-1-big-data-conceptcn/</guid><description>&lt;img src="https://example.com/post/img/24.jpg" alt="Featured image of post Hadoop Chapter 1: Big Data Concept(CN)" />&lt;p>&lt;a class="link" href="https://xn--8mr985eba830aiye.vip/p/hadoop-chapter-1-big-data-concepten/" target="_blank" rel="noopener"
>English Version Portal&lt;/a>&lt;/p>
&lt;h2 id="什么是大数据">什么是大数据
&lt;/h2>&lt;ul>
&lt;li>指体积庞大的数据集合&lt;/li>
&lt;li>随时间呈指数级增长&lt;/li>
&lt;li>传统数据管理工具无法有效存储或处理&lt;/li>
&lt;li>具有海量规模的数据&lt;/li>
&lt;/ul>
&lt;h3 id="大数据类型">大数据类型
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>结构化数据&lt;/strong>：能够以固定格式存储、访问和处理的数据&lt;/li>
&lt;li>&lt;strong>非结构化数据&lt;/strong>：形式或结构未知的数据&lt;/li>
&lt;li>&lt;strong>半结构化数据&lt;/strong>：同时包含两种形式的数据&lt;/li>
&lt;/ul>
&lt;h3 id="大数据特征">大数据特征
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>体量（Volume）&lt;/strong>：数据规模庞大&lt;/li>
&lt;li>&lt;strong>多样性（Variety）&lt;/strong>：数据来源和性质的异构性&lt;/li>
&lt;li>&lt;strong>速度（Velocity）&lt;/strong>：数据生成速率&lt;/li>
&lt;li>&lt;strong>真实性（Veracity）&lt;/strong>：待分析的内容质量&lt;/li>
&lt;/ul>
&lt;h3 id="大数据处理优势">大数据处理优势
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>商业决策&lt;/strong>：可利用外部智能辅助决策&lt;/li>
&lt;li>&lt;strong>客户服务提升&lt;/strong>：改善服务质量&lt;/li>
&lt;li>&lt;strong>风险预警&lt;/strong>：早期识别产品/服务风险&lt;/li>
&lt;li>&lt;strong>运营效率&lt;/strong>：提高运营效能&lt;/li>
&lt;/ul>
&lt;h2 id="hadoop生态系统">Hadoop生态系统
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>数据存储&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>HDFS&lt;/strong>：分布式文件系统&lt;/li>
&lt;li>&lt;strong>HBase&lt;/strong>：列式数据库存储&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>数据处理&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>MapReduce&lt;/strong>：集群计算框架&lt;/li>
&lt;li>&lt;strong>YARN&lt;/strong>：集群资源管理系统&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>数据访问&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Hive&lt;/strong>：SQL查询引擎&lt;/li>
&lt;li>&lt;strong>Pig&lt;/strong>：数据流处理&lt;/li>
&lt;li>&lt;strong>Mahout&lt;/strong>：机器学习库&lt;/li>
&lt;li>&lt;strong>Avro&lt;/strong>：远程过程调用&lt;/li>
&lt;li>&lt;strong>Sqoop&lt;/strong>：数据迁移工具&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>数据管理&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Oozie&lt;/strong>：工作流调度&lt;/li>
&lt;li>&lt;strong>Chukwa&lt;/strong>：系统监控&lt;/li>
&lt;li>&lt;strong>ZooKeeper&lt;/strong>：协调服务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hbase">HBase
&lt;/h3>&lt;ul>
&lt;li>开源非关系型分布式数据库&lt;/li>
&lt;li>NoSQL数据库类型&lt;/li>
&lt;li>支持所有数据类型&lt;/li>
&lt;li>可处理Hadoop生态系统内的任何数据&lt;/li>
&lt;li>运行于HDFS之上，提供类BigTable功能&lt;/li>
&lt;li>使用Java编写，支持REST/Avro/Thrift API&lt;/li>
&lt;/ul>
&lt;h3 id="hive">Hive
&lt;/h3>&lt;ul>
&lt;li>构建于Hadoop之上&lt;/li>
&lt;li>管理大规模分布式数据集&lt;/li>
&lt;li>核心功能：
&lt;ul>
&lt;li>提供ETL（抽取/转换/加载）工具&lt;/li>
&lt;li>存储、查询和分析HDFS/HBase数据&lt;/li>
&lt;li>将SQL转换为MapReduce任务进行海量数据分析&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>特有查询语言HQL（类SQL语法）&lt;/li>
&lt;li>支持SQL用户直接查询&lt;/li>
&lt;li>允许开发者自定义MapReduce处理复杂分析&lt;/li>
&lt;li>局限性：
&lt;ul>
&lt;li>不完全支持事务&lt;/li>
&lt;li>无法修改表数据（更新/删除/插入）&lt;/li>
&lt;li>查询延迟较高&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop">Hadoop
&lt;/h3>&lt;ul>
&lt;li>采用分布式存储处理海量信息&lt;/li>
&lt;li>数据分片存储于多个独立节点&lt;/li>
&lt;li>HDFS专为大规模数据集设计的文件系统&lt;/li>
&lt;li>核心特性：
&lt;ul>
&lt;li>低成本&lt;/li>
&lt;li>高扩展性&lt;/li>
&lt;li>灵活性&lt;/li>
&lt;li>高速处理&lt;/li>
&lt;li>容错机制&lt;/li>
&lt;li>高吞吐量&lt;/li>
&lt;li>最小化网络流量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="storm">Storm
&lt;/h3>&lt;ul>
&lt;li>开源分布式实时计算系统&lt;/li>
&lt;li>简化流式数据的可靠处理&lt;/li>
&lt;li>高性能（单节点每秒百万级处理）&lt;/li>
&lt;li>主要特点：
&lt;ul>
&lt;li>易扩展性&lt;/li>
&lt;li>容错机制&lt;/li>
&lt;li>低延迟处理&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="zookeeper">ZooKeeper
&lt;/h3>&lt;ul>
&lt;li>Hadoop生态系统的协调者&lt;/li>
&lt;li>分布式环境中的服务协调中枢&lt;/li>
&lt;/ul>
&lt;h2 id="hadoop发展历程与版本">Hadoop发展历程与版本
&lt;/h2>&lt;ul>
&lt;li>大数据两大核心问题：
&lt;ul>
&lt;li>海量数据存储&lt;/li>
&lt;li>存储数据处理&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Hadoop作为解决方案包含：
&lt;ul>
&lt;li>HDFS分布式文件系统&lt;/li>
&lt;li>YARN资源管理器&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="发展大事记">发展大事记
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>2002&lt;/strong>：Apache Nutch项目启动&lt;/li>
&lt;li>&lt;strong>2003&lt;/strong>：Google发布GFS论文&lt;/li>
&lt;li>&lt;strong>2004&lt;/strong>：Google发布MapReduce论文&lt;/li>
&lt;li>&lt;strong>2005&lt;/strong>：Nutch分布式文件系统诞生&lt;/li>
&lt;li>&lt;strong>2006&lt;/strong>：Hadoop与HDFS正式发布&lt;/li>
&lt;li>&lt;strong>2007&lt;/strong>：Yahoo部署千节点集群&lt;/li>
&lt;li>&lt;strong>2013&lt;/strong>：Hadoop 2.0发布&lt;/li>
&lt;li>&lt;strong>2017&lt;/strong>：Hadoop 3.0发布&lt;/li>
&lt;/ul>
&lt;h2 id="hadoop发行版评估标准">Hadoop发行版评估标准
&lt;/h2>&lt;h3 id="性能表现">性能表现
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>早期重点&lt;/strong>：高吞吐量&lt;/li>
&lt;li>&lt;strong>当前趋势&lt;/strong>：兼顾低延迟&lt;/li>
&lt;li>低延迟两大关键指标：
&lt;ul>
&lt;li>原生性能&lt;/li>
&lt;li>扩展能力&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="扩展能力">扩展能力
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>文件系统&lt;/strong>：突破单NameNode架构瓶颈，实现分布式元数据管理&lt;/li>
&lt;li>&lt;strong>节点规模&lt;/strong>：支持千节点级扩展&lt;/li>
&lt;li>&lt;strong>存储密度&lt;/strong>：支持高密度磁盘节点扩展&lt;/li>
&lt;/ul>
&lt;h3 id="可靠性">可靠性
&lt;/h3>&lt;p>Apache Hadoop设计具备从单服务器到数千节点的线性扩展能力，并具有高度容错特性。&lt;/p></description></item><item><title>Hadoop Chapter 1: Big Data Concept(EN)</title><link>https://example.com/p/hadoop-chapter-1-big-data-concepten/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/hadoop-chapter-1-big-data-concepten/</guid><description>&lt;img src="https://example.com/post/img/" alt="Featured image of post Hadoop Chapter 1: Big Data Concept(EN)" />&lt;p>&lt;a class="link" href="https://xn--8mr985eba830aiye.vip/p/hadoop-chapter-1-big-data-conceptcn/" target="_blank" rel="noopener"
>中文版传送门&lt;/a>&lt;/p>
&lt;h2 id="what-is-big-data">What is Big Data
&lt;/h2>&lt;ul>
&lt;li>A collection of data that is huge in volume&lt;/li>
&lt;li>growing exponentially with time&lt;/li>
&lt;li>none of traditional data management tools can store it or process it efficiently&lt;/li>
&lt;li>a data but with huge size&lt;/li>
&lt;/ul>
&lt;h3 id="types-of-big-data">Types of Big Data
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Structured&lt;/strong>: Any data that can be stored, accessed and processed in the form of fixed format is termed as a &amp;lsquo;structured&amp;rsquo; data&lt;/li>
&lt;li>&lt;strong>Unstructured&lt;/strong>: Any data with unknown form or the structure is classified as unstructured data.&lt;/li>
&lt;li>&lt;strong>Semi-structured&lt;/strong>: Semi-structured data can contain both the forms of data.&lt;/li>
&lt;/ul>
&lt;h3 id="characteristics-of-big-data">Characteristics of Big Data
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Volume&lt;/strong>: enormous size&lt;/li>
&lt;li>&lt;strong>Variety&lt;/strong>: heterogeneous sources and the nature of data&lt;/li>
&lt;li>&lt;strong>Velocity&lt;/strong>: the speed of generation of data&lt;/li>
&lt;li>&lt;strong>Veracity&lt;/strong>: the content quality that should be analyzed&lt;/li>
&lt;/ul>
&lt;h3 id="advantages-of-big-data-processing">Advantages Of Big Data Processing
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Businesses&lt;/strong> can utilize outside intelligence while taking decisions.&lt;/li>
&lt;li>Improved &lt;strong>customer service&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Early identification&lt;/strong> of risk to the product/services, if any.&lt;/li>
&lt;li>Better &lt;strong>operational efficiency&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="hadoop-ecosystem">Hadoop Ecosystem
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>Data Storage&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>HDFS&lt;/strong>: File System&lt;/li>
&lt;li>&lt;strong>HBase&lt;/strong>: Column DB Storage&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Data Processing&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Map Reduce&lt;/strong>: Cluster Management&lt;/li>
&lt;li>&lt;strong>YARN&lt;/strong>: Cluster &amp;amp; Resource Management&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Data Access&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Hive&lt;/strong>: SQL&lt;/li>
&lt;li>&lt;strong>Pig&lt;/strong>: Dataflow&lt;/li>
&lt;li>&lt;strong>Mahout&lt;/strong>: Machine Learning&lt;/li>
&lt;li>&lt;strong>Avro&lt;/strong>: RPC&lt;/li>
&lt;li>&lt;strong>Sqoop&lt;/strong>: Data Access&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Data Management&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Oozie&lt;/strong>: Workflow Monitoring&lt;/li>
&lt;li>&lt;strong>Chukwa&lt;/strong>: Monitoring&lt;/li>
&lt;li>&lt;strong>ZooKeeper&lt;/strong>: Management&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hbase">HBase
&lt;/h3>&lt;ul>
&lt;li>an open-source, non-relational distributed database&lt;/li>
&lt;li>it is a NoSQL database&lt;/li>
&lt;li>support all types of data&lt;/li>
&lt;li>can handle anything and everything inside Hadoop ecosystem&lt;/li>
&lt;li>run on top of HDFS and provides BigTable-like capabilities&lt;/li>
&lt;li>written in Java, and HBase applications can be written in REST, Avro, and Thrift APIs&lt;/li>
&lt;/ul>
&lt;h3 id="hive">HIVE
&lt;/h3>&lt;ul>
&lt;li>built on Apache Hadoop&lt;/li>
&lt;li>manage large distributed data sets&lt;/li>
&lt;li>provides following features:
&lt;ul>
&lt;li>provides tools to extract/transform/load data(ETL)&lt;/li>
&lt;li>store, query, and analyze large-scale data stored in HDFS(or HBase)&lt;/li>
&lt;li>SQL is converted into MapReduce jobs and run on Hadoop to perform statistical analysis and processing of massive data&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>defines a query language HQL(similar to SQL)&lt;/li>
&lt;li>users familiar with SQL can query data directly
using Hive&lt;/li>
&lt;li>allows mapReducer-savvy developers to develop custom mappers and reducers to handle the complex analysis work&lt;/li>
&lt;li>disadvantages
&lt;ul>
&lt;li>does not correctly support transactions&lt;/li>
&lt;li>cannot modify table data(cannot update, delete, insert)&lt;/li>
&lt;li>slow query speed&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop">Hadoop
&lt;/h3>&lt;ul>
&lt;li>use a distributed approach to store the massive volume of information&lt;/li>
&lt;li>data was divided up and allocated to many individual databases&lt;/li>
&lt;li>HDFS is a specially design file system for storing huge datasets&lt;/li>
&lt;li>main features
&lt;ul>
&lt;li>cost&lt;/li>
&lt;li>scalability&lt;/li>
&lt;li>flexibility&lt;/li>
&lt;li>speed&lt;/li>
&lt;li>fault tolerance&lt;/li>
&lt;li>high throughput&lt;/li>
&lt;li>minimum network traffic&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="storm">STORM
&lt;/h3>&lt;ul>
&lt;li>a free, open source distributed real-time computing system&lt;/li>
&lt;li>simplifies the reliable processing of streaming data&lt;/li>
&lt;li>very fast, one test achieved one million group processing per second on a single node&lt;/li>
&lt;li>storm features:
&lt;ul>
&lt;li>easy to expand&lt;/li>
&lt;li>storm fault tolerance&lt;/li>
&lt;li>low latency&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="zookeeper">Zookeeper
&lt;/h3>&lt;ul>
&lt;li>is the coordinator of any Hadoop job which includes a combination of services in a Hadoop Ecosystem&lt;/li>
&lt;li>coordinates with various services in a distributed environment&lt;/li>
&lt;/ul>
&lt;h2 id="hadoop-history--versions">Hadoop History &amp;amp; Versions
&lt;/h2>&lt;ul>
&lt;li>Two main problems with big data
&lt;ul>
&lt;li>store such large amounts of data&lt;/li>
&lt;li>process the stored data&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Hadoop is the solution to the big data problem&lt;/li>
&lt;li>consists two components
&lt;ul>
&lt;li>Hadoop Distributed File System(HDFS)&lt;/li>
&lt;li>YARN&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-history">Hadoop History
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>2002&lt;/strong>: Apache Nutch was wtarted&lt;/li>
&lt;li>&lt;strong>2003&lt;/strong>: Google publish Google File System paper&lt;/li>
&lt;li>&lt;strong>2004&lt;/strong>: Google released paper on MapReduce&lt;/li>
&lt;li>&lt;strong>2005&lt;/strong>: Nutch Distributed File System was introduced&lt;/li>
&lt;li>&lt;strong>2006&lt;/strong>: Hadoop was introduced along HDFS&lt;/li>
&lt;li>&lt;strong>2007&lt;/strong>: Yahoo runs two cluster of 1000 machines&lt;/li>
&lt;li>&lt;strong>2013&lt;/strong>: Hadoop 2 was released&lt;/li>
&lt;li>&lt;strong>2017&lt;/strong>: Hadoop 3 was released&lt;/li>
&lt;/ul>
&lt;h2 id="hadoop-distribution-evaluation-criteria">Hadoop Distribution evaluation criteria
&lt;/h2>&lt;h3 id="performance">Performance
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>in the early days&lt;/strong>: fast throughput&lt;/li>
&lt;li>&lt;strong>now&lt;/strong>: includes low latency&lt;/li>
&lt;li>recent emphasis on low latency focuses on two key attributes
&lt;ul>
&lt;li>&lt;strong>raw performance&lt;/strong>&lt;/li>
&lt;li>&lt;strong>scalability&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="scalability">Scalability
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>File&lt;/strong> Hadoop&amp;rsquo;s default architecture consists of a single NameNode. Hadoop platform avoids single NameNode bottlenecks and has a distributed metadata architecture&lt;/li>
&lt;li>&lt;strong>Node number&lt;/strong> your chosen Hadoop implementation may need to scale to 1,000 nodes or more&lt;/li>
&lt;li>&lt;strong>Node capacity/density&lt;/strong> you need to scale nodes with higher disk density&lt;/li>
&lt;/ul>
&lt;h3 id="reliability">Reliability
&lt;/h3>&lt;p>Apache Hadoop is designed to scale from a single server to thousands of computers and is highly fault-tolerant&lt;/p></description></item><item><title>Hadoop Chapter 2: Hadoop and Big Data Architecture(CN)</title><link>https://example.com/p/hadoop-chapter-2-hadoop-and-big-data-architecturecn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/hadoop-chapter-2-hadoop-and-big-data-architecturecn/</guid><description>&lt;img src="https://example.com/post/img/25.jpg" alt="Featured image of post Hadoop Chapter 2: Hadoop and Big Data Architecture(CN)" />&lt;h2 id="hadoop-操作系统模式">Hadoop 操作系统模式
&lt;/h2>&lt;p>&lt;strong>Hadoop 四大核心运行模式&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>本地运行模式&lt;/strong>&lt;/li>
&lt;li>&lt;strong>伪分布式运行模式&lt;/strong>&lt;/li>
&lt;li>&lt;strong>完全分布式运行模式&lt;/strong>&lt;/li>
&lt;li>&lt;strong>高可用性(HA)运行模式&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-本地运行模式">Hadoop 本地运行模式
&lt;/h3>&lt;ul>
&lt;li>默认运行模式&lt;/li>
&lt;li>以单个 Java 进程运行&lt;/li>
&lt;li>又称作本地（独立）模式&lt;/li>
&lt;/ul>
&lt;h4 id="模式配置要求">模式配置要求
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>操作系统&lt;/strong>: Windows 或 Linux x64&lt;/li>
&lt;li>&lt;strong>JDK 版本&lt;/strong>: JDK jdk1.8.0_241&lt;/li>
&lt;li>&lt;strong>Hadoop 版本&lt;/strong>: 3.x&lt;/li>
&lt;/ul>
&lt;p>核心配置文件: hadoop-env.sh&lt;/p>
&lt;h3 id="hadoop-伪分布式模式">Hadoop 伪分布式模式
&lt;/h3>&lt;h4 id="模式概述">模式概述
&lt;/h4>&lt;ul>
&lt;li>模拟完全分布式环境
&lt;ul>
&lt;li>单节点运行 Hadoop&lt;/li>
&lt;li>所有 Hadoop 守护进程运行于同一服务器节点&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>包含五大核心进程
&lt;ul>
&lt;li>名称节点（NameNode）&lt;/li>
&lt;li>数据节点（DataNode）&lt;/li>
&lt;li>第二名称节点（SecondaryNameNode）&lt;/li>
&lt;li>资源管理器（ResourceManager）&lt;/li>
&lt;li>节点管理器（NodeManager）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="hdfs-守护进程">HDFS 守护进程
&lt;/h4>&lt;ul>
&lt;li>HDFS 用于存储海量数据&lt;/li>
&lt;li>正常运行所需进程
&lt;ul>
&lt;li>NameNode&lt;/li>
&lt;li>DataNode&lt;/li>
&lt;li>SecondaryNameNode&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>执行 &lt;code>start-dfs.sh&lt;/code> 命令启动 HDFS 守护进程对外提供服务&lt;/li>
&lt;/ul>
&lt;h4 id="yarn-守护进程">Yarn 守护进程
&lt;/h4>&lt;ul>
&lt;li>Hadoop 3.x 资源管理系统&lt;/li>
&lt;li>核心进程
&lt;ul>
&lt;li>ResourceManager&lt;/li>
&lt;li>NodeManager&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>执行 &lt;code>start-yarn.sh&lt;/code> 启动服务&lt;/li>
&lt;/ul>
&lt;h4 id="伪分布式模式配置">伪分布式模式配置
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>HDFS 配置&lt;/strong>
&lt;ul>
&lt;li>&lt;code>core-site.xml&lt;/code>: 配置 NameNode RPC 远程通信地址，默认端口号 &lt;code>8020&lt;/code>&lt;/li>
&lt;li>&lt;code>hdfs-site.xml&lt;/code>: 设置数据块副本数为 1&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>YARN 配置&lt;/strong>
&lt;ul>
&lt;li>&lt;code>mapred-site.xml&lt;/code>: 指定 MapReduce 运行框架为 YARN&lt;/li>
&lt;li>&lt;code>yarn-site.xml&lt;/code>: 配置 ResourceManager 通信地址及 NodeManager 辅助服务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>其他配置&lt;/strong>
&lt;ul>
&lt;li>&lt;code>hadoop-env.sh&lt;/code>: 设置 hadoop.sh 中的 Java 环境变量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-高可用性运行模式">Hadoop 高可用性运行模式
&lt;/h3>&lt;h4 id="hdfs-高可用架构">HDFS 高可用架构
&lt;/h4>&lt;ul>
&lt;li>通过主备双 NameNode 机制提升可用性
&lt;ul>
&lt;li>活跃 NameNode（Active）&lt;/li>
&lt;li>备用 NameNode（Standby/Passive）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>需解决两大核心问题
&lt;ul>
&lt;li>主备 NameNode 状态实时同步&lt;/li>
&lt;li>同一时刻仅允许一个活跃节点&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="ha-架构实现方案">HA 架构实现方案
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>仲裁日志节点方案&lt;/strong>
&lt;ul>
&lt;li>通过 JournalNodes 集群保持主备同步&lt;/li>
&lt;li>活跃 NameNode 将 EditLog 更新至 JournalNodes&lt;/li>
&lt;li>备用节点持续读取 JournalNodes 的 EditLog 变更并应用&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>共享存储方案&lt;/strong>
&lt;ul>
&lt;li>通过共享存储设备保持主备同步&lt;/li>
&lt;li>活跃 NameNode 将命名空间修改记录至共享存储的 EditLog&lt;/li>
&lt;li>备用节点读取共享存储的 EditLog 变更并应用&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>管理员必须配置至少一种隔离机制（fencing）&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-权限管理">Hadoop 权限管理
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>两级访问控制体系&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>系统级&lt;/strong>: 服务级别授权（ServiceLevel Authorization），控制指定服务是否可访问&lt;/li>
&lt;li>&lt;strong>调度器级&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="动态扩容-datanode">动态扩容 DataNode
&lt;/h3>&lt;ul>
&lt;li>在主节点增加主机名配置，并同步至所有 DataNode 节点&lt;/li>
&lt;li>Hadoop 已预制新 DataNode 配置参数，扩容步骤如下:
&lt;ul>
&lt;li>新增主机名至 hosts 文件&lt;/li>
&lt;li>分发 hadoop 安装文件至新节点&lt;/li>
&lt;li>启动新节点服务&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="动态缩容-datanode">动态缩容 DataNode
&lt;/h3>&lt;ul>
&lt;li>创建 exclude 排除文件&lt;/li>
&lt;li>将待下线节点主机名加入排除文件&lt;/li>
&lt;/ul>
&lt;h3 id="负载均衡">负载均衡
&lt;/h3>&lt;p>节点扩容后如需实现负载均衡，需执行平衡命令:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">bin/start-balancer.sh -threshold &lt;span class="m">10&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="hadoop-与大数据计算架构">Hadoop 与大数据计算架构
&lt;/h2>&lt;h3 id="hadoop-大数据生态定位">Hadoop 大数据生态定位
&lt;/h3>&lt;ul>
&lt;li>Hadoop 生态既非编程语言亦非服务&lt;/li>
&lt;li>是解决大数据问题的平台框架&lt;/li>
&lt;li>生态中多数服务围绕四大核心组件扩展
&lt;ul>
&lt;li>分布式存储 HDFS&lt;/li>
&lt;li>资源调度 YARN&lt;/li>
&lt;li>计算框架 MapReduce&lt;/li>
&lt;li>公共组件库 Common&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-实时计算架构">Hadoop 实时计算架构
&lt;/h3>&lt;p>&lt;img src="https://example.com/img/Big_Data_Technology_Ecosystem.jpg"
loading="lazy"
alt="Big Data Technology Ecosystem"
>&lt;/p></description></item><item><title>Hadoop Chapter 2: Hadoop and Big Data Architecture(EN)</title><link>https://example.com/p/hadoop-chapter-2-hadoop-and-big-data-architectureen/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/hadoop-chapter-2-hadoop-and-big-data-architectureen/</guid><description>&lt;img src="https://example.com/post/img/25.jpg" alt="Featured image of post Hadoop Chapter 2: Hadoop and Big Data Architecture(EN)" />&lt;h2 id="hadoop-operating-modes">Hadoop Operating modes
&lt;/h2>&lt;p>&lt;strong>four main modes of Hadoop operation&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Local runtime mode&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Pseudo-distributed operation mode&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Fully distributed operation mode&lt;/strong>&lt;/li>
&lt;li>&lt;strong>High availability(HA) operating mode&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-local-operation-mode">Hadoop Local operation mode
&lt;/h3>&lt;ul>
&lt;li>default mode&lt;/li>
&lt;li>run as a single Java process&lt;/li>
&lt;li>called Local(Standalone) mode&lt;/li>
&lt;/ul>
&lt;h4 id="mode-configuration">Mode Configuration
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>OS&lt;/strong>: Windows or Linux x64&lt;/li>
&lt;li>&lt;strong>The JDK&lt;/strong>: JDK jdk1.8.0_241&lt;/li>
&lt;li>&lt;strong>Hadoop&lt;/strong>: 3.x&lt;/li>
&lt;/ul>
&lt;p>configuration file: hadoop-env.sh&lt;/p>
&lt;h3 id="hadoop-pseudo-distributed-mode">Hadoop pseudo-distributed mode
&lt;/h3>&lt;h4 id="overview">Overview
&lt;/h4>&lt;ul>
&lt;li>simulate a fully distributed environment
&lt;ul>
&lt;li>Hadoop run on a single node&lt;/li>
&lt;li>each Hadoop daemon running on a single server node&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>five process
&lt;ul>
&lt;li>NameNode&lt;/li>
&lt;li>DataNode&lt;/li>
&lt;li>SecondaryNameNode&lt;/li>
&lt;li>ResourceManager&lt;/li>
&lt;li>NodeManager&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="hdfs-daemon-process">HDFS Daemon Process
&lt;/h4>&lt;ul>
&lt;li>HDFS is used to store large amounts of data&lt;/li>
&lt;li>required for the normal operation of the HDFS
&lt;ul>
&lt;li>NameNode&lt;/li>
&lt;li>DataNode&lt;/li>
&lt;li>SecondaryNameNode&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>run the &lt;code>start-dfs.sh&lt;/code> command to start the HDFS daemon process to provide external services&lt;/li>
&lt;/ul>
&lt;h4 id="yarn-daemon-process">Yarn Daemon Process
&lt;/h4>&lt;ul>
&lt;li>the resource management system in Hadoop 3.x&lt;/li>
&lt;li>required
&lt;ul>
&lt;li>ResourceManager&lt;/li>
&lt;li>NodeManager&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>run &lt;code>start-yarn.sh&lt;/code>&lt;/li>
&lt;/ul>
&lt;h4 id="pattern-configurationpseudo-distributed-mode">Pattern Configuration(pseudo-distributed mode)
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>HDFS Configuration&lt;/strong>
&lt;ul>
&lt;li>&lt;code>core-site.xml&lt;/code>: NameNode RPC remote communication address. The default port number is &lt;code>8020&lt;/code>&lt;/li>
&lt;li>&lt;code>HDFS-site.xml&lt;/code>: set the number of data block copies to 1&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>YARN Configuration&lt;/strong>
&lt;ul>
&lt;li>&lt;code>mapred-site.xml&lt;/code>: set the MapReduce operating framework to YARN&lt;/li>
&lt;li>&lt;code>yarn-site,xml&lt;/code>: set the ResourceManager communication address and aux-services of the NodeManager&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Other Configuration&lt;/strong>
&lt;ul>
&lt;li>&lt;code>hadoop-env.sh&lt;/code>: set the java environment variables in &lt;code>hadoop.sh&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-ha-running-mode">Hadoop HA Running Mode
&lt;/h3>&lt;h4 id="hdfs-ha-architecture">HDFS HA Architecture
&lt;/h4>&lt;ul>
&lt;li>
&lt;p>HA architecture solve the problem of NameNode availability by allowing us to have two NameNodes in an active/passive configuration.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Active NameNode&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Standby/Passive NameNode&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Two main issues in maintaining consistency&lt;/p>
&lt;ul>
&lt;li>Active and Standby NameNode should always be in sync with each other&lt;/li>
&lt;li>There should be only one active NameNode at a time&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="implementation-of-ha-architecture">Implementation of HA Architecture
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>Using Quorum Journal Nodes&lt;/strong>
&lt;ul>
&lt;li>JournalNodes helps the standby and active NameNode keep in sync&lt;/li>
&lt;li>The active NameNode is responsible for updating the EditLogs present in the JournalNodes.&lt;/li>
&lt;li>The StandbyNode reads the changes made to the EditLogs in the JournalNode and applies it to its own
namespace in a constant manner.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Using Shared Storage&lt;/strong>
&lt;ul>
&lt;li>A shared storage device helps standby and active NameNode keep in sync&lt;/li>
&lt;li>The active NameNode logs the record of any modification done in its namespace to an EditLog
present in this shared storage.&lt;/li>
&lt;li>The StandbyNode reads the changes made to the EditLogs in this shared
storage and applies it to its own namespace.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The administrator must configure at least one fencing&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-authority-management">Hadoop Authority Management
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Hadoop access control is divided into two levels&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>system level&lt;/strong>: ServiceLevel Authorization. It is used to control whether specified services can be accessed.&lt;/li>
&lt;li>&lt;strong>scheduler level&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="add-datanode">Add DataNode
&lt;/h3>&lt;ul>
&lt;li>Add a host name on the master node and copy it on any DataNode node.&lt;/li>
&lt;li>Hadoop has configured the relevant parameters to the newly added DataNode and started it on the new node. Specific steps are as follows:
&lt;ul>
&lt;li>Increase hostname&lt;/li>
&lt;li>Copy the hadoop installation file&lt;/li>
&lt;li>Start new node&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="reduce-datanode">Reduce DataNode
&lt;/h3>&lt;ul>
&lt;li>Create an exclude file&lt;/li>
&lt;li>Add the node host name to be deleted in the exclude file&lt;/li>
&lt;/ul>
&lt;h3 id="load-balancing">Load Balancing
&lt;/h3>&lt;p>After adding new nodes, if you want to achieve load balancing, you need to use the balance command: &lt;code>bin/start-balancer.sh -threshold 10&lt;/code>&lt;/p>
&lt;h2 id="hadoop-and-big-data-computing-architecture">Hadoop and Big data Computing Architecture
&lt;/h2>&lt;h3 id="hadoop-and-big-data-architecture">Hadoop and Big Data Architecture
&lt;/h3>&lt;ul>
&lt;li>Hadoop Ecosystem is neither a programming language nor a service&lt;/li>
&lt;li>It is a platform or framework which solves big data problems&lt;/li>
&lt;li>Most of the services available in the Hadoop ecosystem are to supplement the main four core components
&lt;ul>
&lt;li>HDFS&lt;/li>
&lt;li>YARN&lt;/li>
&lt;li>MapReduce&lt;/li>
&lt;li>Common&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="hadoop-real-time-computing-architecture">Hadoop Real-time Computing Architecture
&lt;/h3>&lt;p>&lt;img src="https://example.com/img/Big_Data_Technology_Ecosystem.jpg"
loading="lazy"
alt="Big Data Technology Ecosystem"
>&lt;/p></description></item><item><title>故事汇：DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title><link>https://example.com/p/%E6%95%85%E4%BA%8B%E6%B1%87deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/%E6%95%85%E4%BA%8B%E6%B1%87deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning/</guid><description>&lt;img src="https://example.com/post/img/26.jpg" alt="Featured image of post 故事汇：DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning" />&lt;h2 id="知识笔记">知识笔记
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>&lt;strong>多阶段训练(multi-stage training)&lt;/strong>：指模型训练过程中分为多个阶段（阶段间目标或数据不同），每个阶段针对性地优化模型的不同能力，最终提升整体性能。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>冷启动数据(cold start data)&lt;/strong>：指在模型训练初期（或新任务启动时）使用的特定引导数据，用于解决模型初期因缺乏足够信息导致的性能低下或不稳定问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>训练后阶段&lt;/strong>：指在机器学习模型完成训练后的一系列操作，包括模型评估、优化、部署、监控等。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>预训练与训练的差别&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>预训练&lt;/strong>：在大规模通用数据集上进行&lt;/li>
&lt;li>&lt;strong>训练&lt;/strong>：在特定任务上调整模型参数的过程&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>推理导向的强化学习(Reasoning-Oriented RL)&lt;/strong>：动态奖励机制和结构化探索策略&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2501.12948" target="_blank" rel="noopener"
>原文传送门&lt;/a>&lt;/p>
&lt;p>膜拜大佬&lt;/p>
&lt;h2 id="读书笔记">读书笔记
&lt;/h2>&lt;h3 id="abstract">Abstract
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>DeepSeek-R1-Zero&lt;/strong>
&lt;ul>
&lt;li>通过强化学习训练，且没监督微调&lt;/li>
&lt;li>推理能力优秀&lt;/li>
&lt;li>可读性差，语言混合&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>DeepSeek-R1&lt;/strong>
&lt;ul>
&lt;li>在强化学习之前结合多阶段训练和冷启动数据&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="introduction">Introduction
&lt;/h3>&lt;ul>
&lt;li>训练后阶段可提高推理任务的准确性
&lt;ul>
&lt;li>所需计算资源比预训练少&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>OpenAI o1引入了思考时间
&lt;ul>
&lt;li>有效的测试时间缩放的挑战依旧是一个问题&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>使用纯强化学习过程的自我进化，使得DeepSeek-R1-Zero在推理基准测试上与OpenAI-01相当
&lt;ul>
&lt;li>使用DeepSeek-v3-Base作为基础模型&lt;/li>
&lt;li>采用PRPO作为强化学习框架&lt;/li>
&lt;li>可读性差，语言混乱&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>引入DeepSeek-R1。结合少量冷启动数据和多级训练流水线
&lt;ul>
&lt;li>&lt;strong>冷启动数据微调&lt;/strong>：修复基础语言能力
&lt;ul>
&lt;li>收集数千条高质量冷启动数据（例如：人工标注的数学解题步骤、语法规范的写作范文）&lt;/li>
&lt;li>用这些数据对基础模型 &lt;code>DeepSeek-V3-Base&lt;/code> 进行监督微调（SFT）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>推理导向的RL训练&lt;/strong>：提升特定任务的推理能力
&lt;ul>
&lt;li>使用强化学习（如PPO算法）训练模型，奖励函数侧重推理正确性（如解题步骤分、最终答案分）。&lt;/li>
&lt;li>训练接近收敛时，模型能稳定生成正确但可能可读性较差的答案&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>拒绝采样生成新SFT数据&lt;/strong>：从RL模型的结果中提取高质量数据，重新注入监督训练
&lt;ul>
&lt;li>让RL模型的结果中提取高质量数据，重新注入监督训练&lt;/li>
&lt;li>通过规则或奖励模型筛选出推理正确且可读性高的结果（例如保留前10%的优质答案）&lt;/li>
&lt;li>混入新RL数据和原有监督数据&lt;/li>
&lt;li>用混合数据重新微调DeepSeek-V3-Base&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>全场景二次RL训练&lt;/strong>：在多任务竞争中进一步平衡性能
&lt;ul>
&lt;li>输入涵盖所有任务的提示（如同时包含数学题、写作要求、事实问答）&lt;/li>
&lt;li>设计多维度奖励函数，如数学任务：步骤正确性+答案准确性；写作任务：流畅性+语法正确性&lt;/li>
&lt;li>基于混合奖励进行RL训练，迫使模型兼顾多领域性能&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="核心贡献">核心贡献
&lt;/h4>&lt;ol>
&lt;li>
&lt;p>&lt;strong>后训练(Post-Training)&lt;/strong>：直接对基础模型进行大规模强化学习（RL）&lt;/p>
&lt;ol>
&lt;li>创新点
&lt;ol>
&lt;li>跳过监督微调(SFT)&lt;/li>
&lt;li>激励模型自主探索思维链(CoT)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>意义：证明纯RL训练可激发LLM推理能力（无需SFT提供参考答案）&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-TEXT" data-lang="TEXT">&lt;span class="line">&lt;span class="cl">步骤1：将方程改写为 3x² - 2x - 8 = 0
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">步骤2：尝试因式分解 → 失败 → 反思：“可能需要使用求根公式。”
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">步骤3：应用求根公式 x = [2 ± √(4 + 96)] / 6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">步骤4：计算判别式 √100 = 10 → x = (2 ± 10)/6
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">步骤5：验证解是否满足原方程 → 确认 x=2 和 x=-4/3 均为解
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="3">
&lt;li>开发流程
&lt;ol>
&lt;li>&lt;strong>第一阶段RL&lt;/strong>：基于基础模型进行RL训练，奖励函数侧重推理正确性。探索更优推理模式（如数学解题策略、代码调试逻辑）&lt;/li>
&lt;li>&lt;strong>第一阶段SFT&lt;/strong>：混合RL生成的优质推理数据与通用领域SFT数据。固化RL探索到的优质推理模式，并补充非推理能力（如写作、对话）。&lt;/li>
&lt;li>&lt;strong>第二阶段RL&lt;/strong>：引入人类反馈（如人工标注偏好排序）优化奖励模型。对齐人类偏好（如可读性、安全性）&lt;/li>
&lt;li>&lt;strong>第二阶段SFT&lt;/strong>：平衡多任务性能，防止RL过度优化单一领域&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>蒸馏(Distillation)&lt;/strong>：让小模型继承大模型推理能力&lt;/p>
&lt;ol>
&lt;li>&lt;strong>核心思想&lt;/strong>：用大模型生成的推理数据训练小模型，使其超越RL训练的小模型&lt;/li>
&lt;li>降低推理成本，促进小模型实际应用&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="discussion">Discussion
&lt;/h3>&lt;h4 id="蒸馏和强化学习的比较">蒸馏和强化学习的比较
&lt;/h4>&lt;ul>
&lt;li>将更强大的模型提炼成更小的模型可以得到很好的结果，而依赖于大规模RL的模型需要巨大的计算能力，甚至可能达不到提炼的性能&lt;/li>
&lt;li>尽管提炼策略既经济又有效，但要超越智能的界限，可能仍然需要更强大的基础模型和更大规模的强化学习&lt;/li>
&lt;/ul>
&lt;h4 id="未成功的尝试">未成功的尝试
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>过程奖励模型(Process Reward Model, PRM)&lt;/strong>
&lt;ul>
&lt;li>在一般推理中明确定义一个细粒度的步骤是一个挑战&lt;/li>
&lt;li>确定当前中间步骤是否正确是一项具有挑战性的任务&lt;/li>
&lt;li>一旦引入了基于模型的PRM，就不可避免地会导致奖励黑客行为。而重新培训奖励模型需要额外的培训资源，这使整个培训流程变得复杂&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>蒙特卡洛树搜索(Monte Carlo Tree Search, MCTS)&lt;/strong>
&lt;ul>
&lt;li>将答案分解为更小的部分，以允许模型系统地探索解决方案空间&lt;/li>
&lt;li>为了方便这一点，提示模型生成多个标签，这些标签对应于搜索所需的特定推理步骤&lt;/li>
&lt;li>难点：
&lt;ul>
&lt;li>token的生成有很多空间。解决方案：为每个节点设置最大扩展限制，但可能会陷入局部最优&lt;/li>
&lt;li>价值模型直接影响生成的质量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>故事汇：Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data</title><link>https://example.com/p/%E6%95%85%E4%BA%8B%E6%B1%87dynamic-distillation-network-for-cross-domain-few-shot-recognition-with-unlabeled-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/%E6%95%85%E4%BA%8B%E6%B1%87dynamic-distillation-network-for-cross-domain-few-shot-recognition-with-unlabeled-data/</guid><description>&lt;img src="https://example.com/post/img/15.jpg" alt="Featured image of post 故事汇：Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data" />&lt;h2 id="原文链接">原文链接
&lt;/h2>&lt;p>&lt;a class="link" href="https://arxiv.org/pdf/2106.07807" target="_blank" rel="noopener"
>传送门&lt;/a>&lt;/p>
&lt;h2 id="知识清单">知识清单
&lt;/h2>&lt;h3 id="abstract">Abstract
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>小样本学习方法(few-shot learning)&lt;/strong>：让模型仅通过极少量样本（如1-5个样本，称为1-shot或5-shot）快速学习新任务&lt;/li>
&lt;li>&lt;strong>元学习(meta-learning)&lt;/strong>：通过大量相似任务（如分类不同模型）训练模型
&lt;ul>
&lt;li>例如：训练时让模型学习“如何区分5种类别的鸟类”，测试时快速适应“区分5种新鸟类”。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>跨领域小样本学习(cross-domain few-shot)&lt;/strong>：基础数据集与目标数据集来自不同领域（如自然图像→医学影像），且目标数据极少或无标签&lt;/li>
&lt;li>&lt;strong>STARTUP&lt;/strong>：解决跨领域小样本学习中目标数据无标签的问题
&lt;ul>
&lt;li>&lt;strong>自训练(self-training)&lt;/strong>：用预训练教师模型对无标签目标数据生成伪标签（即软标签），再结合少量标注数据训练学生模型&lt;/li>
&lt;li>&lt;strong>固定教师&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>软标签(soft labels)&lt;/strong>：概率分布形式的标签（如[0.7, 0.3]表示“70%概率是类别A”），而非硬标签（如[1, 0]）&lt;/li>
&lt;li>&lt;strong>弱增强(weakly-augmented)&lt;/strong>：对输入数据施加轻微变换的预处理操作，比强增强更温和&lt;/li>
&lt;/ul>
&lt;h3 id="introduction">Introduction
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>&lt;strong>指数移动平均(EMA)&lt;/strong>：通过加权平均更新参数，赋予近期参数更高的权重，同时保留历史参数的衰减影响。&lt;/p>
&lt;ul>
&lt;li>教师网络的参数$\theta_t$由学生网络参数$\theta_s$通过EMA更新。其中$\beta$是衰减率&lt;/li>
&lt;/ul>
&lt;p>$$
\theta_t \leftarrow \beta \cdot \theta_t + (1-\beta)\cdot \theta_s
$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>蒸馏(Distillation)&lt;/strong>：知识蒸馏是一种将“教师模型”的知识迁移到“学生模型”的技术，通常通过让学生模仿教师的输出来实现。核心思想是让学生学习教师的软标签（概率分布），而不仅是真实标签的硬标签。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>教师生成软标签&lt;/strong>：输入数据经教师模型前向传播，输出概率分布（如分类任务的类别概率）&lt;/li>
&lt;li>&lt;strong>学生匹配软标签&lt;/strong>：学生模型对相同（或增强后的）数据输出概率，并通过损失函数（如 KL 散度）逼近教师的输出。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>支持集(Support Set)&lt;/strong>：一小批带有标签的样本，用来“教”模型快速认识新任务中的类别。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>查询集(Query Set)&lt;/strong>：一批需要分类的样本，用来测试模型是否真正学会了新类别。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>注意：支持集和查询集、训练集和测试集是有不同的&lt;/p>
&lt;ol>
&lt;li>训练集/测试集的目的是训练一个模型解决单一固定任务&lt;/li>
&lt;li>支持集/测试集让模型快速适应新任务&lt;/li>
&lt;li>训练集 vs 支持集：如果只用支持集（如每类1张图）训练传统模型，模型会严重过拟合（只会背答案，无法泛化）。支持集必须配合元学习框架，让模型提前掌握“快速学习能力”。&lt;/li>
&lt;li>测试集 vs 查询集：测试集是静态的，任务固定；查询集是动态的，每次任务不同（如今天分类鸟类，明天分类岩石）。查询集的评估目标是“模型能否快速适应新任务”，而非“是否精通某一任务”。&lt;/li>
&lt;/ol>&lt;/blockquote>
&lt;h3 id="related-work">Related Work
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>生成式方法&lt;/strong>：通过生成新样本来扩充数据
&lt;ul>
&lt;li>自己画一些假狗的照片（生成数据），结合真实照片一起训练
&lt;ul>
&lt;li>比如用尺子量新照片和样本照片的&amp;quot;鼻子长度&amp;quot;&amp;ldquo;耳朵形状&amp;quot;等特征，越像狗分越高&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>基于度量的方法&lt;/strong>：核心是学习样本间的相似度计算方式、
&lt;ul>
&lt;li>&lt;strong>Matching Networks&lt;/strong>：把照片变成数学向量，计算相似度&lt;/li>
&lt;li>&lt;strong>Prototypical Networks&lt;/strong>：先计算所有样本的平均特征（比如哈士奇平均有蓝眼睛、竖耳朵），新照片和这个平均值对比&lt;/li>
&lt;li>&lt;strong>Relation Networks&lt;/strong>：让AI自己发明一套「相似度计算公式」，而不是用现成的余弦相似度&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>基于自适应的方法&lt;/strong>：通过参数调整快速适应新任务
&lt;ul>
&lt;li>&lt;strong>MAML&lt;/strong>：提前把模型参数训练得像橡皮泥一样，遇到新任务只需微调几步
&lt;ul>
&lt;li>比如先学会识别动物轮廓，遇到新动物时快速调整细节（斑马条纹/长颈鹿脖子）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>元学习(Meta-learning)&lt;/strong>：“学会学习”的范式，通过多个任务训练模型获取可迁移的知识
&lt;ul>
&lt;li>先让AI玩100个「用5张图认新东西」的小游戏（每个游戏认不同动物）&lt;/li>
&lt;li>AI在这些游戏中总结出经验：比耳朵形状比颜色更重要，先看轮廓再看细节&lt;/li>
&lt;li>遇到新游戏（比如用5张企鹅照片认企鹅），就能快速应用这些经验&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>自训练(Self-training)&lt;/strong>：一种半监督学习方法，其核心思想是通过模型自身的预测结果（伪标签）逐步扩充训练数据
&lt;ul>
&lt;li>&lt;strong>基本流程&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>初始训练&lt;/strong>：使用少量有标签数据训练基础模型（Teacher Model）&lt;/li>
&lt;li>&lt;strong>伪标签生成&lt;/strong>：用该模型预测无标签数据的类别，筛选高置信度预测结果作为伪标签&lt;/li>
&lt;li>&lt;strong>数据扩充&lt;/strong>：将伪标签数据与原始有标签数据合并，重新训练模型（Student Model）&lt;/li>
&lt;li>&lt;strong>迭代优化&lt;/strong>：重复步骤2-3，直至模型收敛或达到终止条件&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>通俗解释&lt;/strong>：假设你是一个学生
&lt;ul>
&lt;li>&lt;strong>第一步&lt;/strong>：老师先教你10道数学题（有标签数据），你学会了基本解法&lt;/li>
&lt;li>&lt;strong>第二步&lt;/strong>：老师布置100道新题（无标签数据），你先用学会的方法做完，并挑出自己最有把握的50道题（高置信度伪标签）&lt;/li>
&lt;li>&lt;strong>第三步&lt;/strong>：把这50道自认为正确的题当作「参考答案」，结合原来的10道题重新复习&lt;/li>
&lt;li>&lt;strong>第四步&lt;/strong>：重复做题→选答案→复习的过程，直到你觉得所有题都会了&lt;/li>
&lt;li>&lt;strong>注意风险&lt;/strong>：如果前几步自己做错了还当成正确答案，后面会越错越离谱（错误累积）。所以老师通常会要求：只相信95分以上的答案（置信度阈值），或者让多个同学互相对答案（多模型协同）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>半监督学习(Semi-supervised Learning)&lt;/strong>：用少量带答案（标签）和大量不带答案的数据（无标签）一起训练模型&lt;/li>
&lt;li>&lt;strong>FixMatch&lt;/strong>：用自信的猜测教自己&lt;/li>
&lt;li>&lt;strong>STARTUP(Self-Training Adaptation Using Pseudo-labels)&lt;/strong>：通过伪标签和自监督对比学习，，利用目标领域的未标注数据提升模型在跨域任务中的性能。
&lt;ul>
&lt;li>&lt;strong>核心问题设定&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>基础域（Source Domain）&lt;/strong>：有大量标注数据（如自然图像）&lt;/li>
&lt;li>&lt;strong>目标域（Target Domain）&lt;/strong>：仅有极少量标注数据（如医学图像），但可能有大量未标注数据。&lt;/li>
&lt;li>&lt;strong>目标&lt;/strong>：让模型从基础域迁移到目标域，仅用少量目标域标注样本实现高精度分类。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>方法流程&lt;/strong>
&lt;ol>
&lt;li>&lt;strong>预训练模型&lt;/strong>：在基础域上训练一个分类模型（如ResNet），作为固定（Frozen）的预训练模型。&lt;/li>
&lt;li>&lt;strong>生成伪标签&lt;/strong>：使用预训练模型对目标域的未标注数据生成伪标签（即预测结果作为“软标签”）&lt;/li>
&lt;li>&lt;strong>联合训练&lt;/strong>：结合基础域的标注数据（真实标签）和目标域的伪标签数据，重新训练模型。&lt;/li>
&lt;li>&lt;strong>自监督对比学习&lt;/strong>：在未标注数据上加入对比损失（如SimCLR），学习对数据增强鲁棒的特征表示。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>大白话&lt;/strong>：先蒙答案，蒙完再改，改的时候还要自我检查&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="品细品">品，细品
&lt;/h2>&lt;h3 id="abstract-1">Abstract
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>现有工作&lt;/strong>：依赖于在与目标数据集同域的大型基础数据集上进行网络元学习
&lt;ul>
&lt;li>&lt;strong>缺陷&lt;/strong>：在基础域和目标域存在显著差异的跨域小样本学习效果不行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>本文提出&lt;/strong>：使用动态蒸馏，有效利用新/基础数据集的未标记图像
&lt;ul>
&lt;li>通过教师网络对未标记图像的弱增强版本生成预测&lt;/li>
&lt;li>通过学生网络对同一图像的强增强版本进行预测&lt;/li>
&lt;li>通过一致性正则化约束两者匹配&lt;/li>
&lt;li>教师网络的参数通过学生网络参数的指数移动平均动态更新&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="introduction-1">Introduction
&lt;/h3>&lt;h4 id="典型小样本学习跨领域小样本学习本文提出的新设定的区别">典型小样本学习、跨领域小样本学习、本文提出的新设定的区别
&lt;/h4>&lt;p>&lt;img src="https://example.com/img/%e4%b8%89%e7%a7%8d%e5%b0%8f%e6%a0%b7%e6%9c%ac.png"
loading="lazy"
alt="三者差别"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>典型小样本学习（左）&lt;/strong>
&lt;ul>
&lt;li>基础数据集和目标数据集来自同一领域&lt;/li>
&lt;li>类别互不相交&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>跨领域小样本学习（中）&lt;/strong>
&lt;ul>
&lt;li>基础数据集与目标数据存在领域差异&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>本文提出的设定（右）&lt;/strong>
&lt;ul>
&lt;li>在元训练阶段引入无标签目标数据&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>通俗易懂的解释&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>典型小样本学习（左）&lt;/strong>：你是一个只会画“猫和狗”的画家，现在要快速学会画“鸟和鱼”。
&lt;ul>
&lt;li>&lt;strong>基础训练&lt;/strong>：你之前画过大量&lt;strong>不同品种的猫和狗&lt;/strong>（同一领域：动物）&lt;/li>
&lt;li>&lt;strong>小样本任务&lt;/strong>：客户给你看&lt;strong>1张鸟的照片&lt;/strong>和&lt;strong>1张鱼的照片&lt;/strong>（支持集），要求你画出这两种动物的其他姿势（查询集）&lt;/li>
&lt;li>&lt;strong>关键点&lt;/strong>
&lt;ul>
&lt;li>你学的（猫狗）和要画的（鸟鱼）都是&lt;strong>动物&lt;/strong>，只是品种不同（同一领域，类别不相交）&lt;/li>
&lt;li>你靠之前的动物绘画经验（如毛发、眼睛的画法），快速模仿鸟和鱼的特征&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>类比总结&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>领域相同&lt;/strong>：全是动物&lt;/li>
&lt;li>&lt;strong>挑战&lt;/strong>：用旧知识（画猫狗）解决同类新问题（画鸟鱼）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>跨领域小样本学习（中）&lt;/strong>：你是一个画“自然风景”的画家，现在要快速学会画“抽象几何图形”
&lt;ul>
&lt;li>&lt;strong>基础训练&lt;/strong>：你之前画过大量&lt;strong>山川、河流、树木&lt;/strong>（自然领域）&lt;/li>
&lt;li>&lt;strong>小样本任务&lt;/strong>：客户给你看&lt;strong>1个三角形&lt;/strong>和&lt;strong>1个圆形&lt;/strong>（支持集），要求你画出其他几何图形（如六边形）&lt;/li>
&lt;li>&lt;strong>关键点&lt;/strong>
&lt;ul>
&lt;li>自然风景（曲线、光影）和几何图形（直线、对称）属于&lt;strong>完全不同的领域&lt;/strong>&lt;/li>
&lt;li>你只能用画风景的经验（如颜色搭配）去“硬猜”如何画几何图形，效果可能很差&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>类比总结&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>领域不同&lt;/strong>：自然风景 vs. 几何图形&lt;/li>
&lt;li>&lt;strong>挑战&lt;/strong>：旧经验（自然）和新任务（几何）毫无关联，从头适应难如登天&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>本文提出的新设定&lt;/strong>：你是一个画“自然风景”的画家，但客户提前给了你一堆&lt;strong>未标注的几何图形草稿&lt;/strong>，现在要快速学会画“抽象几何图形”
&lt;ul>
&lt;li>&lt;strong>基础训练&lt;/strong>
&lt;ul>
&lt;li>你画过大量自然风景（带标签的源数据）&lt;/li>
&lt;li>还看过很多&lt;strong>未标注的几何图形草稿&lt;/strong>（无标签目标数据），虽然不知道它们具体是什么，但熟悉了直线、对称等特征。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>小样本任务&lt;/strong>：客户给你看&lt;strong>1个三角形&lt;/strong>和&lt;strong>1个圆形&lt;/strong>（支持集），要求你画出其他几何图形。&lt;/li>
&lt;li>&lt;strong>关键点&lt;/strong>
&lt;ul>
&lt;li>未标注的几何草稿让你提前适应了“几何领域”的风格（如直线比曲线多）。&lt;/li>
&lt;li>结合自然风景的绘画技巧（如色彩搭配）和几何领域的特征，你能更快画出客户想要的图形。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>类比总结&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>领域不同&lt;/strong>：自然风景（源） vs. 几何图形（目标）。&lt;/li>
&lt;li>&lt;strong>秘密武器&lt;/strong>：提前看过未标注的几何草稿（无标签目标数据），相当于“预习”了新领域的规则。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="related-work-1">Related Work
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>&lt;strong>Few-shot classification&lt;/strong>: 少样本分类可分为三大类：生成式、基于度量、基于适应。早期少样本学习工作基于元学习&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Self-training&lt;/strong>: 自训练通过训练学生模型来模仿教师模型的预测&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Semi-supervised Learning&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>核心思想&lt;/strong>：同时利用少量有标签数据和大量无标签数据进行训练&lt;/li>
&lt;li>&lt;strong>FixMatch方法核心逻辑&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>弱增强生成伪标签&lt;/strong>：对无标签图像做弱增强（如平移、旋转），用模型预测其伪标签。&lt;/li>
&lt;li>&lt;strong>强增强训练一致性&lt;/strong>：若伪标签置信度高，则对同一图像做强增强（如颜色失真、模糊），并让模型预测与伪标签一致。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>作者改进方法&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>一致性正则化&lt;/strong>：强制模型对同一数据的不同增强版本（如弱增强和强增强）输出一致。与FixMatch类似，但不假设无标签数据与有标签数据同领域。&lt;/li>
&lt;li>&lt;strong>均值教师网络&lt;/strong>：用教师模型（Teacher Network）生成伪标签，学生模型（Student Network）学习。教师模型是学生模型的指数移动平均（EMA），稳定性更高，伪标签噪声更小。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cross-domain few-shot learning&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>现有最先进方法在跨域少样本学习上难以达到理想准确率&lt;/li>
&lt;li>&lt;strong>现有方法：STARTUP&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>方法&lt;/strong>：用预训练模型为未标记的目标域数据生成伪标签，结合基础域标注数据和目标域伪标签训练模型。&lt;/li>
&lt;li>&lt;strong>局限&lt;/strong>：伪标签依赖&lt;strong>固定预训练模型&lt;/strong>，若模型不适应目标领域，错误会累积（如用自然图像预训练的模型直接标注医学影像）。需要额外设计&lt;strong>自监督对比损失&lt;/strong>（如SimCLR），增加计算复杂度。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>本文方法：动态蒸馏(Dynamic Distillation)&lt;/strong>
&lt;ol>
&lt;li>&lt;strong>监督学习&lt;/strong>：使用标记的基础数据集优化监督交叉熵损失。&lt;/li>
&lt;li>&lt;strong>动态蒸馏&lt;/strong>
&lt;ul>
&lt;li>对目标图像的弱增强版本，用教师网络生成预测&lt;/li>
&lt;li>对同一图像的强增强版本，由学生网络生成预测&lt;/li>
&lt;li>通过蒸馏损失约束两者预测分布一致&lt;/li>
&lt;li>教师预测应用温度锐化以鼓励学生输出低熵预测&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>参数更新&lt;/strong>：学生网络通过监督损失和蒸馏损失联合优化，教师网络参数采用学生网络的指数移动平均更新。&lt;/li>
&lt;li>&lt;strong>少样本评估&lt;/strong>：仅需在少样本支撑集上学习新分类器头，直接对查询集进行评估。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://example.com/img/%e5%8a%a8%e6%80%81%e8%92%b8%e9%a6%8f.jpeg"
loading="lazy"
alt="动态蒸馏"
>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="methodology">Methodology
&lt;/h3>&lt;h4 id="proposed-method">Proposed Method
&lt;/h4>&lt;ul>
&lt;li>
&lt;p>&lt;strong>Encoder&lt;/strong>：通过知识蒸馏方法，在源数据集和目标数据集上联合训练基础编码器。将嵌入网络表示为$f_s$，它将输入图像$x$编码为一个$d$维向量$f_s(x)$。我们在$f_s$上添加一个分类头$g_s$，用于从嵌入向量中预测$n_c$个逻辑值(logits)，其中$n_c$是基数据集（base dataset）的类别总数。由于基数据集中的样本标签已知，我们计算监督交叉熵损失：
$$
l_{CE}(y,p)=H(y,p)\
p=Softmax(g_s(f_s(x)))
H(a,b)=-alog b
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>核心目标&lt;/strong>：通过&lt;strong>知识蒸馏&lt;/strong>（类似“老师教学生”），让编码器同时学习源数据集（如动物图片）和目标数据集（如医疗X光片）的特征，提升跨域任务的泛化能力。&lt;/li>
&lt;li>&lt;strong>模型结构&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>编码器&lt;/strong>$f_s$：将输入图像（如一张x光片）转换为一个向量，这个向量代表图像的特征（如形状、纹理）&lt;/li>
&lt;li>&lt;strong>分类器头&lt;/strong>$g_s$：接在编码器后面，将特征向量映射到类别概率&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>监督损失&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>输入&lt;/strong>：源数据集（带标签）的图片&lt;/li>
&lt;li>计算步骤：
&lt;ol>
&lt;li>编码器提取特征 $\rightarrow f_s(x)$&lt;/li>
&lt;li>分类器预测类别概率 $\rightarrow p = Softmax(g_s(f_s(x)))$&lt;/li>
&lt;li>用交叉熵损失$l_{CE}$衡量预测概率$p$和真实标签$y$的差距&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>通俗解释&lt;/strong>：如果真实标签是“肺炎”，但模型预测概率为0.1，损失会很大；如果预测概率是0.9，损失就小。这个过程迫使编码器和分类器学习源数据集的分类能力。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Dynamic distillation&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>核心思想&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>教师-学生模式&lt;/strong>：教师网络生成“参考答案”（伪标签），学生网络通过模仿教师来学习&lt;/li>
&lt;li>&lt;strong>动态更新&lt;/strong>：教师网络不是固定的，而是随着学生网络的训练逐步更新，类似“老师跟着学生一起进步”。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>关键步骤&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>数据增强&lt;/strong>：迫使模型对不同增强版本预测一致，提升鲁棒性&lt;/li>
&lt;li>&lt;strong>伪标签生成&lt;/strong>：
&lt;ol>
&lt;li>教师网络处理弱增强图像$x_i^w$，生成软目标$p_i^w$（概率分布，而非硬标签）&lt;/li>
&lt;li>学生网络处理强增强图像$x_i^s$，生成预测$p_i^s$&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>&lt;strong>损失计算&lt;/strong>
&lt;ul>
&lt;li>监督损失$l_{CE}$：在源数据（带标签）上计算交叉熵损失&lt;/li>
&lt;li>蒸馏损失$l_U$：迫使学生网络的预测$p_i^s$与教师网络的伪标签$p_i^w$一致&lt;/li>
&lt;li>总损失是两者的加权和（$\lambda$控制未标记数据的重要性）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>教师网络更新&lt;/strong>
&lt;ul>
&lt;li>教师网络的权重是学生网络权重的历史平均（动量更新）&lt;/li>
&lt;li>动态更新使得教师网络更稳定，避免伪标签噪声过大。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="experiments">Experiments
&lt;/h3>&lt;h4 id="experimental-setup">Experimental Setup
&lt;/h4>&lt;ul>
&lt;li>
&lt;p>&lt;strong>数据集&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>基数据集(Base Dataset)&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>miniImageNet&lt;/strong>：从&lt;code>ImageNet&lt;/code>中选取的100个类别，每个类别含600张图像（总计60,000张），类别覆盖通用物体（如动物、日常用品），用于监督预训练。&lt;/li>
&lt;li>&lt;strong>tieredImageNet&lt;/strong>：更大的基数据集，包含608个类别（34个超类），分为训练（351类）、验证（97类）、测试（160类），用于验证模型对大规模数据的泛化性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>新领域数据集(Novel Dataset)&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>CropDisease&lt;/strong>：农业植物病害图像，类别与miniImageNet的语义差异显著（领域差异大）。&lt;/li>
&lt;li>&lt;strong>EuroSAT&lt;/strong>：遥感卫星图像（土地利用分类），与自然图像分布不同（低分辨率、多光谱特征）。&lt;/li>
&lt;li>&lt;strong>ISIC&lt;/strong>：皮肤病医学影像（皮肤镜图像），模态差异明显（纹理、颜色分布独特）。&lt;/li>
&lt;li>&lt;strong>ChestX&lt;/strong>：胸部X光影像（肺炎分类），灰度图像且解剖结构复杂。&lt;/li>
&lt;li>&lt;strong>选择依据&lt;/strong>：按与miniImageNet的领域差异递增排序（CropDisease差异最小，ChestX差异最大），用于测试跨域小样本泛化性。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>数据划分协议&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>无标签集&lt;/strong>$D_U$：从每个新数据集中随机抽取20%样本（例如，CropDisease若含1,000张，则取200张作为$D_U$）&lt;/li>
&lt;li>&lt;strong>评估集&lt;/strong>：剩余80%样本用于&lt;code>5-way K-shot&lt;/code> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>分类任务（支持集采样K张/类，查询集评估）。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>小样本评估&lt;/strong>：在支持集上训练逻辑回归分类器，在查询集测试性能&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验1：BSCD-FSL基准测试duibi&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://example.com/img/dc-table1.png"
loading="lazy"
alt="Table 1"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型对比&lt;/strong>：
&lt;ul>
&lt;li>传统元学习（MetaOpt、MAML、ProtoNet）&lt;/li>
&lt;li>自监督学习（SimCLR）&lt;/li>
&lt;li>混合方法（Transfer+SimCLR）&lt;/li>
&lt;li>最新跨域方法（STARTUP）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>训练设置&lt;/strong>
&lt;ul>
&lt;li>基础数据集：miniImageNet（80 类）&lt;/li>
&lt;li>目标数据集：未标记数据（20% 目标集）&lt;/li>
&lt;li>主干网络：ResNet-10（miniImageNet）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>评估指标&lt;/strong>：5 分类 1-shot/5-shot 准确率（600 次运行的均值 ±95% 置信区间）&lt;/li>
&lt;li>&lt;strong>优点&lt;/strong>
&lt;ul>
&lt;li>Ours 在所有数据集上均超越 STARTUP（平均提升 5.5%~8.8%）&lt;/li>
&lt;li>动态教师网络生成的伪标签随训练优化，优于固定教师（STARTUP）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验2：tiredImageNet基础数据实验&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://example.com/img/dc-table2.png"
loading="lazy"
alt="Table 2"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>目的&lt;/strong>：验证方法在大规模基础数据集上的泛化性&lt;/li>
&lt;li>&lt;strong>数据集&lt;/strong>：tieredImageNet（608 类，划分为 34 个超级类别）&lt;/li>
&lt;li>&lt;strong>模型对比&lt;/strong>：
&lt;ul>
&lt;li>基于 miniImageNet 的基准&lt;/li>
&lt;li>基于 tieredImageNet 的基线&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>训练设置&lt;/strong>
&lt;ul>
&lt;li>更大主干网络：ResNet-18&lt;/li>
&lt;li>元训练策略：MAML 框架&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>评估指标&lt;/strong>：5 分类 1-shot/5-shot 准确率&lt;/li>
&lt;li>&lt;strong>关键发现&lt;/strong>：
&lt;ul>
&lt;li>使用 tieredImageNet 预训练未显著提升性能（对比 miniImageNet）&lt;/li>
&lt;li>验证跨域少样本学习中&lt;strong>数据质量＞数据量&lt;/strong>的假设&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验3：相似域少样本性能&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://example.com/img/dc-table3.png"
loading="lazy"
alt="Table 3"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>目的&lt;/strong>：验证方法在同域 / 相似域的有效性&lt;/li>
&lt;li>&lt;strong>数据集&lt;/strong>
&lt;ul>
&lt;li>miniImageNet（同域）&lt;/li>
&lt;li>tieredImageNet（相似域）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>训练设置&lt;/strong>：
&lt;ul>
&lt;li>未标记数据来自目标域测试集的 20%&lt;/li>
&lt;li>主干网络：ResNet-10（miniImageNet）、ResNet-18（tieredImageNet）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>对比对象&lt;/strong>：
&lt;ul>
&lt;li>Transfer（仅监督训练）&lt;/li>
&lt;li>STARTUP（同域无效）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>关键发现&lt;/strong>：
&lt;ul>
&lt;li>Ours 在同域任务中仍优于 STARTUP（tieredImageNet 1-shot 提升 7.7%）&lt;/li>
&lt;li>动态蒸馏对域差异不敏感，兼具跨域和同域适应性&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>实验4：动态蒸馏效果分析&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://example.com/img/dc-table4.png"
loading="lazy"
alt="Table 4"
>&lt;/p>
&lt;p>&lt;img src="https://example.com/img/dc-figure3.png"
loading="lazy"
>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>目的&lt;/strong>：揭示动态蒸馏如何优化特征表示&lt;/li>
&lt;li>&lt;strong>量化分析&lt;/strong>（表 4）：
&lt;ul>
&lt;li>方法：K 均值聚类 + V-measure 评分&lt;/li>
&lt;li>指标：真实标签与聚类结果的一致性（V-score）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>可视化分析&lt;/strong>（图 3）：
&lt;ul>
&lt;li>方法：t-SNE 降维展示特征分布&lt;/li>
&lt;li>对比：Transfer 基准 vs Ours&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>关键发现&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>聚类质量&lt;/strong>：Ours 在 EuroSAT（85.2%）和 CropDisease（91.3%）上 V-score 最高&lt;/li>
&lt;li>&lt;strong>特征分离&lt;/strong>：可视化显示 Ours 生成的嵌入具有更好的类间区分性&lt;/li>
&lt;li>&lt;strong>机制验证&lt;/strong>：蒸馏损失隐式促进特征聚类，无需显式对比学习&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>5-way指在小样本学习任务中，对5个类别进行分类。K-shot指每个分类提供k个带标签的样本作为训练支持集&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>