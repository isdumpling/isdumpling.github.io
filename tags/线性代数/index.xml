<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>线性代数 on 一只饺子</title><link>https://example.com/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</link><description>Recent content in 线性代数 on 一只饺子</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>一只饺子</copyright><atom:link href="https://example.com/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>对于机器学习公式的例子</title><link>https://example.com/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/</guid><description>&lt;img src="https://example.com/post/img/3.jpg" alt="Featured image of post 对于机器学习公式的例子" />&lt;h2 id="得到权重的值">得到权重的值
&lt;/h2>&lt;p>以下是一个 &lt;strong>单层神经网络（感知机）&lt;/strong> 的完整示例，通过 &lt;strong>手动模拟训练过程&lt;/strong>，展示如何从数据中学习权重。我们以 &lt;strong>房价预测&lt;/strong> 为例，假设数据仅包含一个样本，目标是让模型学会调整权重和偏置。&lt;/p>
&lt;h3 id="问题设定">&lt;strong>问题设定&lt;/strong>
&lt;/h3>&lt;h4 id="输入特征">&lt;strong>输入特征&lt;/strong>
&lt;/h4>&lt;ul>
&lt;li>$ x_1 $（面积）：1（标准化后的值，如100平方米）&lt;/li>
&lt;li>$ x_2 $（房龄）：1（标准化后的值，如5年）&lt;/li>
&lt;/ul>
&lt;h4 id="真实输出">&lt;strong>真实输出&lt;/strong>
&lt;/h4>&lt;ul>
&lt;li>$ y_{\text{true}} = 3 $（单位：万元）&lt;/li>
&lt;/ul>
&lt;h4 id="模型结构">&lt;strong>模型结构&lt;/strong>
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>线性模型&lt;/strong>：$ y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b $&lt;/li>
&lt;li>&lt;strong>初始参数&lt;/strong>（随机初始化）：
&lt;ul>
&lt;li>权重：$ w_1 = 0.5 $, $ w_2 = -0.3 $&lt;/li>
&lt;li>偏置：$ b = 0.2 $&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="目标">&lt;strong>目标&lt;/strong>
&lt;/h4>&lt;p>通过梯度下降，调整 $ w_1, w_2, b $，使得 $ y_{\text{pred}} $ 接近真实值 3。&lt;/p>
&lt;h3 id="训练过程">&lt;strong>训练过程&lt;/strong>
&lt;/h3>&lt;h4 id="前向传播计算预测值">&lt;strong>前向传播（计算预测值）&lt;/strong>
&lt;/h4>&lt;p>$$
y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b = 0.5 \times 1 + (-0.3) \times 1 + 0.2 = 0.5 - 0.3 + 0.2 = 0.4
$$
此时预测值为 0.4 万元，与真实值 3 相差较大。&lt;/p>
&lt;h4 id="计算损失均方误差">&lt;strong>计算损失（均方误差）&lt;/strong>
&lt;/h4>&lt;p>$$
\text{Loss} = (y_{\text{true}} - y_{\text{pred}})^2 = (3 - 0.4)^2 = 6.76
$$&lt;/p>
&lt;h4 id="反向传播计算梯度">&lt;strong>反向传播（计算梯度）&lt;/strong>
&lt;/h4>&lt;p>对每个参数求偏导（链式法则）：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>损失对 $ w_1 $ 的梯度&lt;/strong>：
$$
\frac{\partial \text{Loss}}{\partial w_1} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_1 = 2(0.4 - 3) \times 1 = -5.2
$$&lt;/li>
&lt;li>&lt;strong>损失对 $ w_2 $ 的梯度&lt;/strong>：
$$
\frac{\partial \text{Loss}}{\partial w_2} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_2 = 2(0.4 - 3) \times 1 = -5.2
$$&lt;/li>
&lt;li>&lt;strong>损失对 $ b $ 的梯度&lt;/strong>：
$$
\frac{\partial \text{Loss}}{\partial b} = 2(y_{\text{pred}} - y_{\text{true}}) = 2(0.4 - 3) = -5.2
$$&lt;/li>
&lt;/ul>
&lt;h4 id="更新参数梯度下降">&lt;strong>更新参数（梯度下降）&lt;/strong>
&lt;/h4>&lt;p>设定学习率 $ \eta = 0.1 $，更新规则：
$$
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial \text{Loss}}{\partial w}
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>更新 $ w_1 $&lt;/strong>：
$$
w_1 = 0.5 - 0.1 \times (-5.2) = 0.5 + 0.52 = 1.02
$$&lt;/li>
&lt;li>&lt;strong>更新 $ w_2 $&lt;/strong>：
$$
w_2 = -0.3 - 0.1 \times (-5.2) = -0.3 + 0.52 = 0.22
$$&lt;/li>
&lt;li>&lt;strong>更新 $ b $&lt;/strong>：
$$
b = 0.2 - 0.1 \times (-5.2) = 0.2 + 0.52 = 0.72
$$&lt;/li>
&lt;/ul>
&lt;h3 id="更新后的预测">&lt;strong>更新后的预测&lt;/strong>
&lt;/h3>&lt;p>使用新参数重新计算预测值：
$$
y_{\text{pred}} = 1.02 \times 1 + 0.22 \times 1 + 0.72 = 1.02 + 0.22 + 0.72 = 1.96
$$
损失更新为：
$$
\text{Loss} = (3 - 1.96)^2 = 1.08
$$
&lt;strong>仅一次迭代，损失从 6.76 下降到 1.08&lt;/strong>，说明权重调整有效。&lt;/p>
&lt;h3 id="多轮迭代后的结果">&lt;strong>多轮迭代后的结果&lt;/strong>
&lt;/h3>&lt;p>重复上述过程（假设学习率不变）：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>迭代次数&lt;/th>
&lt;th>$ w_1 $&lt;/th>
&lt;th>$ w_2 $&lt;/th>
&lt;th>$ b $&lt;/th>
&lt;th>$ y_{\text{pred}} $&lt;/th>
&lt;th>Loss&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>0.5&lt;/td>
&lt;td>-0.3&lt;/td>
&lt;td>0.2&lt;/td>
&lt;td>0.4&lt;/td>
&lt;td>6.76&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1.02&lt;/td>
&lt;td>0.22&lt;/td>
&lt;td>0.72&lt;/td>
&lt;td>1.96&lt;/td>
&lt;td>1.08&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>1.45&lt;/td>
&lt;td>0.65&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>2.60&lt;/td>
&lt;td>0.16&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>1.68&lt;/td>
&lt;td>0.89&lt;/td>
&lt;td>1.43&lt;/td>
&lt;td>2.96&lt;/td>
&lt;td>0.0016&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>经过3次迭代，预测值 $ y_{\text{pred}} = 2.96 $ 接近真实值3，损失降至0.0016。&lt;/p>
&lt;h3 id="关键结论">&lt;strong>关键结论&lt;/strong>
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>权重的本质&lt;/strong>：模型通过梯度下降，沿着损失减小的方向调整权重，逐步逼近真实值。&lt;/li>
&lt;li>&lt;strong>学习率的作用&lt;/strong>：学习率 $ \eta $ 控制参数更新步幅（过大可能导致震荡，过小收敛慢）。&lt;/li>
&lt;li>&lt;strong>实际训练&lt;/strong>：真实场景中需使用大量数据分批训练，而非单个样本。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>附：Python代码模拟&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 初始参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">w1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_true&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">eta&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 前向传播&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y_pred&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">w1&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">x1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">w2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">x2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">y_true&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">y_pred&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 计算梯度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dL_dw1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_pred&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">y_true&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">x1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dL_dw2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_pred&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">y_true&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">x2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dL_db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_pred&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">y_true&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 更新参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w1&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">eta&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dL_dw1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w2&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">eta&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dL_dw2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">b&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">eta&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dL_db&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Epoch &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">epoch&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: w1=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">w1&lt;/span>&lt;span class="si">:&lt;/span>&lt;span class="s2">.2f&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">, w2=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">w2&lt;/span>&lt;span class="si">:&lt;/span>&lt;span class="s2">.2f&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">, b=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="si">:&lt;/span>&lt;span class="s2">.2f&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">, y_pred=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">y_pred&lt;/span>&lt;span class="si">:&lt;/span>&lt;span class="s2">.2f&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">, Loss=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="si">:&lt;/span>&lt;span class="s2">.4f&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="y--b--sum_i-c_i--textsigmoidb_i--sum_j-w_ij-x_j">$y = b + \sum_i c_i , \text{sigmoid}(b_i + \sum_j w_{ij} x_j)$
&lt;/h2>&lt;p>假设我们要根据房屋的两个特征预测房价&lt;/p>
&lt;ul>
&lt;li>&lt;strong>特征1($x_1$)&lt;/strong>：面积（平方米）&lt;/li>
&lt;li>&lt;strong>特征2($x_2$)&lt;/strong>：房龄（年）&lt;/li>
&lt;/ul>
&lt;p>我们设计一个简单的神经网络，结构如下：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>输入层：&lt;/strong> 两个特征（$x_1,x_2$）&lt;/li>
&lt;li>&lt;strong>隐藏层&lt;/strong>： 2个神经元（$i=1,2$）&lt;/li>
&lt;li>&lt;strong>输出层&lt;/strong>： 1个输出（房价$y$）&lt;/li>
&lt;/ul>
&lt;h3 id="step-1设定参数值">step 1：设定参数值
&lt;/h3>&lt;p>假设模型已经训练完成，参数如下：&lt;/p>
&lt;p>&lt;strong>隐藏层参数&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>神经元&lt;/th>
&lt;th>权重$w_{i1}$（面积权重）&lt;/th>
&lt;th>权重$w_{i2}$(房龄权重)&lt;/th>
&lt;th>偏置$b_i$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1$(i=1)$&lt;/td>
&lt;td>0.8&lt;/td>
&lt;td>-0.2&lt;/td>
&lt;td>0.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2$(i=2)$&lt;/td>
&lt;td>0.5&lt;/td>
&lt;td>-0.6&lt;/td>
&lt;td>-0.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>输出层参数&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>权重$c_i$&lt;/th>
&lt;th>偏置$b$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$c_1=10$&lt;/td>
&lt;td>$b=5$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$c_2=-8$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="step-2输出数据">step 2：输出数据
&lt;/h3>&lt;p>假设有一套房子的特征值为：&lt;/p>
&lt;ul>
&lt;li>面积$x_1=100m^2$&lt;/li>
&lt;li>房龄$x_2=5年$&lt;/li>
&lt;/ul>
&lt;h3 id="step-3计算隐藏层输出">step 3：计算隐藏层输出
&lt;/h3>&lt;p>对每个隐藏层神经元，计算$z_i,=,b_i,+,w_{i1}x_1,+,w_{i2}x_2$，然后通过$sigmoid$激活函数得到$a_i=sigmoid(z_i)$&lt;/p>
&lt;p>&lt;strong>神经元1($i=1$)的计算&lt;/strong>&lt;/p>
&lt;p>$$
\begin{aligned}
z_1 = b_1 + w_{11}x_1 + w_{12}x_2 = 0.5 + 0.8 \times 100 + (-0.2) \times 5 = 0.5 + 80 - 1 = 79.5\
a_1 = \text{sigmoid}(79.5) = \frac{1}{1 + e^{-79.5}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$&lt;/p>
&lt;p>&lt;strong>神经元2($i=2$)的计算&lt;/strong>&lt;/p>
&lt;p>$$
\begin{aligned}
z_2 = b_2 + w_{21}x_1 + w_{22}x_2 = -0.3 + 0.5 \times 100 + (-0.6) \times 5 = -0.3 + 50 - 3 = 46.7\
a_2 = \text{sigmoid}(46.7) = \frac{1}{1 + e^{-46.7}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$&lt;/p>
&lt;h3 id="step-4计算输出层结果">step 4：计算输出层结果
&lt;/h3>&lt;p>$$
y = b + c_1 a_1 + c_2 a_2 = 5 + 10 \times 1.0 + (-8) \times 1.0 = 5 + 10 - 8 = 7
$$&lt;/p>
&lt;h2 id="lthetaapprox-ltheta-ltheta---thetagfrac12theta-thetaththeta-theta-">$L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})$
&lt;/h2>&lt;p>由于&lt;del>线性代数学艺不精&lt;/del>热爱线性代数，重新推导这个公式&lt;/p>
&lt;ol>
&lt;li>&lt;strong>回忆一维泰勒展开&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>例如，在$x&amp;rsquo;$附件展开$f(x)$
$$
f(x)\approx f(x&amp;rsquo;)+f&amp;rsquo;(x&amp;rsquo;)(x-x&amp;rsquo;)+\frac{1}{2}f&amp;quot;(x&amp;rsquo;)(x-x&amp;rsquo;)^2
$$&lt;/p>
&lt;p>对于泰勒展开公式：
$$
f(x_0,x)=\sum_{i=0}^n \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i
$$&lt;/p>
&lt;ol start="2">
&lt;li>&lt;strong>扩展到多维情况（参数$\theta$是向量）&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>$L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})$&lt;/p>
&lt;p>在多维情况下，参数是向量$\theta = [\theta_1,\theta_2,&amp;hellip;\theta_n]^T$，梯度$g$是一阶导数的推广&lt;/p>
&lt;p>对于&lt;strong>Hessian&lt;/strong>矩阵，是多元函数的二阶偏导数构成的矩阵
$$
H = \nabla^2 f = \begin{bmatrix}
\frac{\partial^2 f}{\partial \theta_1^2} &amp;amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_1 \partial \theta_n} \
\frac{\partial^2 f}{\partial \theta_2 \partial \theta_1} &amp;amp; \frac{\partial^2 f}{\partial \theta_2^2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_2 \partial \theta_n} \
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \
\frac{\partial^2 f}{\partial \theta_n \partial \theta_1} &amp;amp; \frac{\partial^2 f}{\partial \theta_n \partial \theta_2} &amp;amp; \cdots &amp;amp; \frac{\partial^2 f}{\partial \theta_n^2}
\end{bmatrix}
$$&lt;/p>
&lt;p>&lt;strong>为什么需要转置 $(\theta - \theta&amp;rsquo;)^\top$？&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>维度匹配&lt;/strong>：假设 $\theta$ 是 $n \times 1$ 向量，梯度 $g$ 也是 $n \times 1$，Hessian H$ $H$是 $n \times n$。
&lt;ul>
&lt;li>一阶项：$(\theta - \theta &amp;lsquo;)^Tg$是$n \times 1$向量，梯度$g$也是$n\times 1$，Hessian $H$是$n \times n$（标量）
&lt;ul>
&lt;li>一阶项：$(\theta - \theta &amp;lsquo;)g$是$1\times n*n\times n * n \times 1=1\times 1$（标量）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>数学必要性&lt;/strong>：转置确保矩阵乘法维度相容。&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>&lt;strong>一个具体的例子&lt;/strong>&lt;/li>
&lt;/ol>
&lt;h5 id="1-定义函数">&lt;strong>1. 定义函数&lt;/strong>
&lt;/h5>&lt;p>设损失函数 $L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2$，参考点 $\theta&amp;rsquo; = [0, 0]^\top$。&lt;/p>
&lt;h5 id="2-计算梯度-g">&lt;strong>2. 计算梯度 $g$&lt;/strong>
&lt;/h5>&lt;p>$$
g = \nabla L(\theta&amp;rsquo;) = \begin{bmatrix} 2\theta_1 + \theta_2 \ 4\theta_2 + \theta_1 \end{bmatrix} \bigg|_{\theta&amp;rsquo;=[0,0]} = \begin{bmatrix} 0 \ 0 \end{bmatrix}
$$&lt;/p>
&lt;h5 id="3-计算-hessian-矩阵-h">&lt;strong>3. 计算 Hessian 矩阵 $H$&lt;/strong>
&lt;/h5>&lt;p>$$
H = \nabla^2 L(\theta&amp;rsquo;) = \begin{bmatrix}
\frac{\partial^2 L}{\partial \theta_1^2} &amp;amp; \frac{\partial^2 L}{\partial \theta_1 \partial \theta_2} \
\frac{\partial^2 L}{\partial \theta_2 \partial \theta_1} &amp;amp; \frac{\partial^2 L}{\partial \theta_2^2}
\end{bmatrix} = \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix}
$$&lt;/p>
&lt;h5 id="4-泰勒展开公式">&lt;strong>4. 泰勒展开公式&lt;/strong>
&lt;/h5>&lt;p>在 $\theta&amp;rsquo; = [0, 0]^\top$ 处展开：
$$
L(\theta) \approx \underbrace{0}&lt;em>{L(\theta&amp;rsquo;)} + \underbrace{(\theta - 0)^\top \begin{bmatrix} 0 \ 0 \end{bmatrix}}&lt;/em>{\text{一阶项}} + \frac{1}{2}(\theta - 0)^\top \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix} (\theta - 0)
$$&lt;/p>
&lt;p>化简后：
$$
L(\theta) \approx \frac{1}{2}\theta^\top \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 4 \end{bmatrix} \theta = \frac{1}{2}(2\theta_1^2 + 2\theta_1\theta_2 + 4\theta_2^2)
$$&lt;/p>
&lt;p>展开后与原函数一致：
$$
L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2
$$&lt;/p></description></item><item><title>机器学习（李宏毅）笔记 4：局部最小值与鞍点</title><link>https://example.com/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-4%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC%E4%B8%8E%E9%9E%8D%E7%82%B9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-4%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC%E4%B8%8E%E9%9E%8D%E7%82%B9/</guid><description>&lt;img src="https://example.com/post/img/7.jpg" alt="Featured image of post 机器学习（李宏毅）笔记 4：局部最小值与鞍点" />&lt;h2 id="critical-point情况">Critical Point情况
&lt;/h2>&lt;ol>
&lt;li>&lt;strong>局部最小值（local minima）&lt;/strong>。如果是**卡在local minima,那可能就没有路可以走了，**因为四周都比较高，你现在所在的位置已经是最低的点，loss最低的点了，往四周走 loss都会比较高，你会不知道怎么走到其他地方去。&lt;/li>
&lt;li>&lt;strong>鞍点（saddle point）&lt;/strong>。（如图可看出，左右是比红点高，前后比红点低，红点既不是local minima,也不是local maxima的地方）如果是卡在saddle point，saddle point旁边还是有其他路可以让你的loss更低的，你只要逃离saddle point，你就有可能让你的loss更低。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://example.com/img/%e5%b1%80%e9%83%a8%e6%9c%80%e5%b0%8f%e5%80%bc.png"
loading="lazy"
alt="局部最小值"
>&lt;/p>
&lt;p>&lt;img src="https://example.com/img/%e9%9e%8d%e7%82%b9.png"
loading="lazy"
alt="鞍点"
>&lt;/p>
&lt;h2 id="确定critical-point类型">确定Critical Point类型
&lt;/h2>&lt;p>使用泰勒级数近似
$$
L(\theta)\approx L(\theta ^{&amp;rsquo;})+L(\theta - \theta^{&amp;rsquo;})g+\frac{1}{2}(\theta-\theta^{&amp;rsquo;})^{T}H(\theta-\theta ^{&amp;rsquo;})
$$
&lt;strong>计算Hessian矩阵的特征值&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>正定&lt;/strong>（所有特征值 &amp;gt; 0）→ &lt;strong>局部极小值&lt;/strong>&lt;/li>
&lt;li>&lt;strong>负定&lt;/strong>（所有特征值 &amp;lt; 0）→ &lt;strong>局部极大值&lt;/strong>&lt;/li>
&lt;li>&lt;strong>不定&lt;/strong>（特征值有正有负）→ &lt;strong>鞍点&lt;/strong>&lt;/li>
&lt;li>&lt;strong>半正定/半负定&lt;/strong>（存在零特征值）→ &lt;strong>需更高阶分析&lt;/strong>（如退化临界点）&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>具体例子&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;strong>案例1：正定Hessian → 局部极小值&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>$$
H=\begin{bmatrix}
2 &amp;amp; 1 \
1 &amp;amp; 2
\end{bmatrix}
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>特征值&lt;/strong>：3和1（均 &amp;gt; 0）→ 正定&lt;/li>
&lt;li>&lt;strong>结论&lt;/strong>：局部极小值&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>&lt;strong>案例2：负定Hessian → 局部极大值&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>$$
H=\begin{bmatrix}
-2 &amp;amp; 0 \
0 &amp;amp; -2
\end{bmatrix}
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>特征值&lt;/strong>：-2 和 -2（均 &amp;lt; 0）→ 负定&lt;/li>
&lt;li>&lt;strong>结论&lt;/strong>：局部极大值。&lt;/li>
&lt;/ul>
&lt;ol start="3">
&lt;li>&lt;strong>案例3：不定Hessian → 鞍点&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>$$
H = \begin{bmatrix}
2 &amp;amp; 0 \
0 &amp;amp; -2
\end{bmatrix}
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>特征值&lt;/strong>：2 和 -2（有正有负）→ 不定&lt;/li>
&lt;li>&lt;strong>结论&lt;/strong>：鞍点&lt;/li>
&lt;/ul>
&lt;h2 id="逃离saddle-point">逃离Saddle Point
&lt;/h2>&lt;h3 id="利用hessian矩阵逃离鞍点saddle-point">利用Hessian矩阵逃离鞍点（Saddle Point）
&lt;/h3>&lt;p>&lt;strong>核心思想&lt;/strong>：通过Hessian矩阵的负特征值对应的特征向量方向更新参数，使优化方向逃离鞍点。&lt;/p>
&lt;p>&lt;strong>具体步骤&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>1. 检测鞍点&lt;/strong>
&lt;ul>
&lt;li>计算梯度$\nabla f$，若$||\nabla f|| \approx 0$，则可能为临界点&lt;/li>
&lt;li>计算Hessian矩阵$H$，并分析其特征值&lt;/li>
&lt;li>若存在负特征值，则为鞍点。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>2. 找到负曲率方向&lt;/strong>
&lt;ul>
&lt;li>对Hessian矩阵进行特征分解，找到最小特征值$\lambda_{\min} &amp;lt; 0$及其对应的特征向量$v_{\min}$。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>3. 沿负曲率方向更新参数&lt;/strong>
&lt;ul>
&lt;li>选择步长$\eta$（通常与学习率相关），沿$v_{min}$方向更新参数：$\theta_{new}=\theta_{old}+\eta v_{min}$&lt;/li>
&lt;li>**验证方向：**通过计算$\theta_{new}=\theta_{old}+\eta v_{min}$和$\theta_{new}=\theta_{old}-\eta v_{min}$，选择使函数值下降的方向&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>4. 迭代直至逃离&lt;/strong>
&lt;ul>
&lt;li>重复步骤1-3，直到梯度不再接近0或Hessian矩阵变为正定（局部最小值）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="利用momentum逃离鞍点">利用momentum逃离鞍点
&lt;/h3>&lt;h4 id="动量法的基本原理">&lt;strong>动量法的基本原理&lt;/strong>
&lt;/h4>&lt;p>动量法（Momentum）是梯度下降的改进版本，通过&lt;strong>积累历史梯度方向&lt;/strong>加速收敛并抑制振荡。其更新公式为：
$$
v_t = \beta v_{t-1} + (1-\beta) \nabla f(\theta_t)
$$&lt;/p>
&lt;p>$$
\theta{t+1} = \theta_t - \eta v_t
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>动量系数&lt;/strong>：$$\beta \in [0, 1)$$，通常取0.9或0.99。&lt;/li>
&lt;li>&lt;strong>核心思想&lt;/strong>：梯度方向被赋予“惯性”，在平坦区域（如鞍点）积累动量，帮助逃离。&lt;/li>
&lt;/ul>
&lt;h4 id="动量如何帮助逃离鞍点">&lt;strong>动量如何帮助逃离鞍点？&lt;/strong>
&lt;/h4>&lt;p>鞍点的特征：梯度$$\nabla f \approx 0$$，但Hessian矩阵存在&lt;strong>负曲率方向&lt;/strong>。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>梯度下降的缺陷&lt;/strong>：在鞍点附近，梯度接近零，参数更新停滞。&lt;/li>
&lt;li>&lt;strong>动量的优势&lt;/strong>：
&lt;ol>
&lt;li>&lt;strong>历史梯度累积&lt;/strong>：即使当前梯度为零，动量项$$v_t$$仍可能保留之前方向的惯性。&lt;/li>
&lt;li>&lt;strong>噪声放大&lt;/strong>：随机梯度（如SGD的小批量噪声）会被动量放大，打破对称性。&lt;/li>
&lt;li>&lt;strong>负曲率方向探索&lt;/strong>：动量推动参数沿历史梯度方向移动，可能进入负曲率区域。&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h4 id="动量逃离鞍点的数学解释">&lt;strong>动量逃离鞍点的数学解释&lt;/strong>
&lt;/h4>&lt;p>假设在鞍点附近，梯度方向存在随机扰动$$\epsilon_t$$（如小批量噪声）：
$$
\nabla f(\theta_t) = \epsilon_t \quad (\mathbb{E}[\epsilon_t] = 0, \text{Var}(\epsilon_t) = \sigma^2)
$$
动量更新公式变为：
$$
v_t = \beta v_{t-1} + (1-\beta) \epsilon_t
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>动量积累&lt;/strong>：经过$$k$$步后，动量近似为：
$$
v_t \approx (1-\beta) \sum_{i=0}^{k} \beta^{k-i} \epsilon_i
$$&lt;/li>
&lt;li>&lt;strong>逃离机制&lt;/strong>：噪声的加权和可能指向负曲率方向，使参数突破鞍点。&lt;/li>
&lt;/ul>
&lt;h4 id="具体步骤与算法">&lt;strong>具体步骤与算法&lt;/strong>
&lt;/h4>&lt;h5 id="步骤1初始化动量">&lt;strong>步骤1：初始化动量&lt;/strong>
&lt;/h5>&lt;p>设置初始动量$$v_0 = 0$$，选择动量系数$\beta$和学习率$\eta$。&lt;/p>
&lt;h5 id="步骤2迭代更新">&lt;strong>步骤2：迭代更新&lt;/strong>
&lt;/h5>&lt;p>对每次迭代$t$：&lt;/p>
&lt;ol>
&lt;li>计算当前梯度$\nabla f(\theta_t)$（可含噪声，如SGD）。&lt;/li>
&lt;li>更新动量：
$$
v_t = \beta v_{t-1} + (1-\beta) \nabla f(\theta_t)
$$&lt;/li>
&lt;li>更新参数：
$$
\theta_{t+1} = \theta_t - \eta v_t
$$&lt;/li>
&lt;/ol>
&lt;h5 id="步骤3逃离鞍点的动态">&lt;strong>步骤3：逃离鞍点的动态&lt;/strong>
&lt;/h5>&lt;ul>
&lt;li>&lt;strong>鞍点附近&lt;/strong>：梯度$\nabla f \approx 0$，但动量$v_t$可能因历史梯度或噪声不为零。&lt;/li>
&lt;li>&lt;strong>持续更新&lt;/strong>：动量推动参数离开平坦区域，进入梯度较大的区域。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h4 id="实验案例动量法逃离二元鞍点">&lt;strong>实验案例：动量法逃离二元鞍点&lt;/strong>
&lt;/h4>&lt;h5 id="目标函数">&lt;strong>目标函数&lt;/strong>
&lt;/h5>&lt;p>$$
f(x, y) = x^2 - y^2
$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>鞍点&lt;/strong>：$(0, 0)$，Hessian矩阵特征值为$2$和$-2$。&lt;/li>
&lt;/ul>
&lt;h5 id="参数设置">&lt;strong>参数设置&lt;/strong>
&lt;/h5>&lt;ul>
&lt;li>初始点：$(0.1, 0.1)$，学习率$\eta = 0.1$，动量系数$\beta = 0.9$。&lt;/li>
&lt;/ul>
&lt;h5 id="迭代过程">&lt;strong>迭代过程&lt;/strong>
&lt;/h5>&lt;ol>
&lt;li>&lt;strong>第1步&lt;/strong>：梯度$\nabla f = (0.2, -0.2)$，动量$v_1 = 0.1 \times (0.2, -0.2)$，更新后点$(0.08, 0.12)$。&lt;/li>
&lt;li>&lt;strong>第2步&lt;/strong>：梯度$\nabla f = (0.16, -0.24)$，动量$v_2 = 0.9v_1 + 0.1 \times (0.16, -0.24)$，更新后点$(0.064, 0.144)$。&lt;/li>
&lt;li>&lt;strong>持续迭代&lt;/strong>：动量在$y$方向逐渐积累，推动逃离鞍点区域。&lt;/li>
&lt;/ol>
&lt;h4 id="动量法的理论支持">&lt;strong>动量法的理论支持&lt;/strong>
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>收敛性证明&lt;/strong>：在凸函数中，动量法可加速收敛（Nesterov加速）。&lt;/li>
&lt;li>&lt;strong>逃离鞍点能力&lt;/strong>：
&lt;ul>
&lt;li>&lt;strong>随机梯度（SGD）&lt;/strong>：噪声+动量可概率性逃离鞍点（Ge et al., 2015）。&lt;/li>
&lt;li>&lt;strong>确定性梯度&lt;/strong>：动量法需依赖Hessian的负曲率方向隐含在历史梯度中。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="与其他方法的对比">&lt;strong>与其他方法的对比&lt;/strong>
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>方法&lt;/strong>&lt;/th>
&lt;th>&lt;strong>逃离鞍点机制&lt;/strong>&lt;/th>
&lt;th>&lt;strong>计算成本&lt;/strong>&lt;/th>
&lt;th>&lt;strong>适用场景&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>动量法&lt;/strong>&lt;/td>
&lt;td>历史梯度惯性 + 噪声放大&lt;/td>
&lt;td>低（一阶）&lt;/td>
&lt;td>高维、随机优化（如深度学习）&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Hessian矩阵法&lt;/strong>&lt;/td>
&lt;td>显式利用负曲率方向&lt;/td>
&lt;td>高（二阶）&lt;/td>
&lt;td>低维、确定性优化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>SGD + 扰动&lt;/strong>&lt;/td>
&lt;td>纯随机噪声探索&lt;/td>
&lt;td>低（一阶）&lt;/td>
&lt;td>大规模非凸优化&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="实际应用技巧">&lt;strong>实际应用技巧&lt;/strong>
&lt;/h4>&lt;ul>
&lt;li>&lt;strong>动量系数选择&lt;/strong>：$\beta$越大，惯性越强，但可能“冲过头”。常用$\beta=0.9$。&lt;/li>
&lt;li>&lt;strong>与自适应方法结合&lt;/strong>：如Adam（动量+RMSProp），平衡方向与步长。&lt;/li>
&lt;li>&lt;strong>学习率调整&lt;/strong>：在鞍点附近可短暂增大学习率以加速逃离。&lt;/li>
&lt;/ul>
&lt;h4 id="优缺点分析">&lt;strong>优缺点分析&lt;/strong>
&lt;/h4>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>优点&lt;/strong>&lt;/th>
&lt;th>&lt;strong>缺点&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>低计算成本，适合高维问题。&lt;/td>
&lt;td>无显式二阶信息，依赖噪声或历史梯度。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>天然抗噪声，适合随机优化。&lt;/td>
&lt;td>对某些鞍点（如高阶退化点）可能失效。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>易与其他优化器结合（如Adam）。&lt;/td>
&lt;td>需调参（$\beta$, $\eta$）。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item></channel></rss>