<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="批次（batch）与动量（momentum）\r批次\r小批量梯度下降（Mini-batch Gradient Descent） 在训练模型时，不直接使用全部数据计算梯度，而是将数据划分为多个小批量（batch），每次用一个batch的数据计算损失（Loss）和梯度，并更新模型参数。 相比全量梯度下降（计算所有数据），减少内存占用和计算量，同时比随机梯度下降（单个样本）更稳定。 Batch（批次）与Epoch（轮次） Batch：将训练数据分成若干固定大小的子集（如B个样本），每个子集称为一个batch。 Epoch：完整遍历一次全部训练数据的过程（即所有batch被计算一遍）。每个epoch结束后，模型完成一次完整训练。 关系：1个epoch = 所有batch依次计算并更新参数。 参数更新机制 逐batch更新：每个batch计算一次Loss和梯度后，立即更新参数（而非累积所有batch的梯度再更新）。 优点：加快收敛速度，避免全量数据计算的资源瓶颈。 Shuffle 作用：在每个epoch开始前，随机打乱训练数据的顺序，再划分batch。 目的：防止模型因数据顺序产生偏差（如学习到数据排列规律），增强泛化能力。 结果：每个epoch的batch组成不同，提升训练随机性。 "><title>机器学习（李宏毅）笔记 5：批次与动量</title>
<link rel=canonical href=https://example.com/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-5%E6%89%B9%E6%AC%A1%E4%B8%8E%E5%8A%A8%E9%87%8F/><link rel=stylesheet href=/scss/style.min.2deeca980d2b0d58c1f28cdfd20b96c62f8357ef57495cd2d99f0054e39d1d6f.css><meta property='og:title' content="机器学习（李宏毅）笔记 5：批次与动量"><meta property='og:description' content="批次（batch）与动量（momentum）\r批次\r小批量梯度下降（Mini-batch Gradient Descent） 在训练模型时，不直接使用全部数据计算梯度，而是将数据划分为多个小批量（batch），每次用一个batch的数据计算损失（Loss）和梯度，并更新模型参数。 相比全量梯度下降（计算所有数据），减少内存占用和计算量，同时比随机梯度下降（单个样本）更稳定。 Batch（批次）与Epoch（轮次） Batch：将训练数据分成若干固定大小的子集（如B个样本），每个子集称为一个batch。 Epoch：完整遍历一次全部训练数据的过程（即所有batch被计算一遍）。每个epoch结束后，模型完成一次完整训练。 关系：1个epoch = 所有batch依次计算并更新参数。 参数更新机制 逐batch更新：每个batch计算一次Loss和梯度后，立即更新参数（而非累积所有batch的梯度再更新）。 优点：加快收敛速度，避免全量数据计算的资源瓶颈。 Shuffle 作用：在每个epoch开始前，随机打乱训练数据的顺序，再划分batch。 目的：防止模型因数据顺序产生偏差（如学习到数据排列规律），增强泛化能力。 结果：每个epoch的batch组成不同，提升训练随机性。 "><meta property='og:url' content='https://example.com/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-5%E6%89%B9%E6%AC%A1%E4%B8%8E%E5%8A%A8%E9%87%8F/'><meta property='og:site_name' content='一只饺子'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='机器学习'><meta property='article:modified_time' content='2025-03-11T15:19:48+08:00'><meta property='og:image' content='https://example.com/post/img/8.jpg'><meta name=twitter:title content="机器学习（李宏毅）笔记 5：批次与动量"><meta name=twitter:description content="批次（batch）与动量（momentum）\r批次\r小批量梯度下降（Mini-batch Gradient Descent） 在训练模型时，不直接使用全部数据计算梯度，而是将数据划分为多个小批量（batch），每次用一个batch的数据计算损失（Loss）和梯度，并更新模型参数。 相比全量梯度下降（计算所有数据），减少内存占用和计算量，同时比随机梯度下降（单个样本）更稳定。 Batch（批次）与Epoch（轮次） Batch：将训练数据分成若干固定大小的子集（如B个样本），每个子集称为一个batch。 Epoch：完整遍历一次全部训练数据的过程（即所有batch被计算一遍）。每个epoch结束后，模型完成一次完整训练。 关系：1个epoch = 所有batch依次计算并更新参数。 参数更新机制 逐batch更新：每个batch计算一次Loss和梯度后，立即更新参数（而非累积所有batch的梯度再更新）。 优点：加快收敛速度，避免全量数据计算的资源瓶颈。 Shuffle 作用：在每个epoch开始前，随机打乱训练数据的顺序，再划分batch。 目的：防止模型因数据顺序产生偏差（如学习到数据排列规律），增强泛化能力。 结果：每个epoch的batch组成不同，提升训练随机性。 "><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://example.com/post/img/8.jpg'><link rel="shortcut icon" href=/img/icon.svg><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/canvas-nest.js/2.0.4/canvas-nest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/1fc88c9ce946a5621185d27c512d6ae_hu_bc1d6b840fc37823.jpg width=300 height=328 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>一只饺子</a></h1><h2 class=site-description>奶龙也是龙</h2></div></header><ol class=menu-social><li><a href=https://github.com/isdumpling target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/about><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://example.com/en/>English</option><option value=https://example.com/ selected>中文</option><option value=https://example.com/ar/>عربي</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#批次batch与动量momentum>批次（batch）与动量（momentum）</a><ol><li><a href=#批次>批次</a></li><li><a href=#small-batch-vs-large-batch>Small batch vs Large batch</a></li></ol></li><li><a href=#自动调整学习率>自动调整学习率</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-5%E6%89%B9%E6%AC%A1%E4%B8%8E%E5%8A%A8%E9%87%8F/><img src=/post/img/8.jpg loading=lazy alt="Featured image of post 机器学习（李宏毅）笔记 5：批次与动量"></a></div><div class=article-details><header class=article-category><a href=/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-5%E6%89%B9%E6%AC%A1%E4%B8%8E%E5%8A%A8%E9%87%8F/>机器学习（李宏毅）笔记 5：批次与动量</a></h2></div><footer class=article-time>最后修改:
<time class=article-time--updated datetime="2025-03-11 15:19:48 +0800 CST" title="2025-03-11 15:19:48 +0800 CST">2025-03-11</time><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 2 分钟</time></div></footer></div></header><section class=article-content><h2 id=批次batch与动量momentum>批次（batch）与动量（momentum）</h2><h3 id=批次>批次</h3><ol><li><strong>小批量梯度下降（Mini-batch Gradient Descent）</strong><ol><li>在训练模型时，不直接使用全部数据计算梯度，而是将数据划分为多个小批量（batch），每次用一个batch的数据计算损失（Loss）和梯度，并更新模型参数。</li><li>相比全量梯度下降（计算所有数据），减少内存占用和计算量，同时比随机梯度下降（单个样本）更稳定。</li></ol></li><li><strong>Batch（批次）与Epoch（轮次）</strong><ol><li><strong>Batch</strong>：将训练数据分成若干固定大小的子集（如B个样本），每个子集称为一个batch。</li><li><strong>Epoch</strong>：完整遍历一次全部训练数据的过程（即所有batch被计算一遍）。每个epoch结束后，模型完成一次完整训练。</li><li><strong>关系</strong>：1个epoch = 所有batch依次计算并更新参数。</li></ol></li><li><strong>参数更新机制</strong><ol><li><strong>逐batch更新</strong>：每个batch计算一次Loss和梯度后，立即更新参数（而非累积所有batch的梯度再更新）。</li><li><strong>优点</strong>：加快收敛速度，避免全量数据计算的资源瓶颈。</li></ol></li><li><strong>Shuffle</strong><ol><li><strong>作用</strong>：在每个epoch开始前，随机打乱训练数据的顺序，再划分batch。</li><li><strong>目的</strong>：防止模型因数据顺序产生偏差（如学习到数据排列规律），增强泛化能力。</li><li><strong>结果</strong>：每个epoch的batch组成不同，提升训练随机性。</li></ol></li></ol><p><img src=/img/batch.png loading=lazy></p><p>为什么训练时需要用batch？</p><p>参数更新更快，每看一笔资料即会更新一次参数</p><p><img src=/img/batch%e6%af%94%e8%be%83.png loading=lazy alt="左侧无batch，右侧的batch size为1"></p><h3 id=small-batch-vs-large-batch>Small batch vs Large batch</h3><ul><li>没有平行运算时，Small Batch比Large Batch更有效</li><li>有平行运算时，Small Batch与Large Batch运算时间没有太大差距，除非大的超出一定界限</li><li>在一个epoch时间内，Large Batch比Small Batch更快，Large Batch更有效率</li><li>Small Batch比较陡，Large Batch比较稳定</li><li>比较noisy的batch size比比较stable 的batch size在训练和测试时占有优势</li></ul><h2 id=自动调整学习率>自动调整学习率</h2><p>随着参数的更新，loss值逐渐变小并保持在一定值不再下降</p><p><img src=/img/%e8%b0%83%e6%95%b4%e5%ad%a6%e4%b9%a0%e7%8e%87.png loading=lazy></p><p>将gradient decent做的更好的方法是设置每一个参数的学习效率</p><ul><li>如果在某一个方向上，gradient值很小（比较平稳），那么应该把学习效率调高；</li><li>如果在某一个方向上，gradient值很大（比较陡峭），那么应该把学习效率调低。</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 2025-03-11 15:19 CST</span></section><section class=article-pageviews><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/></svg>
<span>页面浏览量<span id=busuanzi_value_page_pv>Loading</span></span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-1%E9%A2%84%E6%B5%8B%E6%9C%AC%E9%A2%91%E9%81%93%E8%A7%82%E6%B5%8B%E4%BA%BA%E6%95%B0%E4%B8%8A/><div class=article-image><img src=/post/img/4.jpg loading=lazy data-key data-hash=/post/img/4.jpg></div><div class=article-details><h2 class=article-title>机器学习（李宏毅）笔记 1：预测本频道观测人数（上）</h2></div></a></article><article class=has-image><a href=/p/pytorch%E8%AF%AD%E6%B3%95/><div class=article-image><img src=/post/img/12.jpg loading=lazy data-key data-hash=/post/img/12.jpg></div><div class=article-details><h2 class=article-title>PyTorch语法</h2></div></a></article><article class=has-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA1%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/><div class=article-image><img src=/post/img/13.jpg loading=lazy data-key data-hash=/post/img/13.jpg></div><div class=article-details><h2 class=article-title>Pytorch实践（刘二大人）1：线性模型</h2></div></a></article><article class=has-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA2%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/><div class=article-image><img src=/post/img/14.jpg loading=lazy data-key data-hash=/post/img/14.jpg></div><div class=article-details><h2 class=article-title>Pytorch实践（刘二大人）2：梯度下降算法</h2></div></a></article><article class=has-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/><div class=article-image><img src=/post/img/15.jpg loading=lazy data-key data-hash=/post/img/15.jpg></div><div class=article-details><h2 class=article-title>Pytorch实践（刘二大人）3：反向传播</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2025 一只饺子</section><section class=powerby><div class=site-stats><span class=stat-item><i class="fas fa-users"></i> 访客数：<span id=busuanzi_value_site_uv>Loading</span>
</span><span class=stat-item><i class="fas fa-eye"></i> 访问量：<span id=busuanzi_value_site_pv>Loading</span>
</span><span class=stat-item><a href=https://eu.umami.is/share/y6kXo4CE3oYHXQ37/farb.github.io target=_blank rel=noopener><i class="fas fa-chart-line"></i> 详细统计</a></span></div><br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><!doctype html><html><head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css><script src=https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js></script><script src=https://cdn.jsdelivr.net/npm/meting@2.0.1/dist/Meting.min.js></script></head><body><div class=demo><div id=player1></div></div><script>var ap=new APlayer({element:document.getElementById("player1"),fixed:!0,autoplay:!1,mini:!0,theme:"#f8f4fc",loop:"all",order:"random",preload:"auto",volume:.4,mutex:!0,listFolded:!0,listMaxHeight:"500px",lrcType:0,music:[{name:"奶龙の小曲",artist:"我是奶龙",url:"/music/我是奶龙 (今夜星光闪闪)(DJ版) - 奶龙.mp3",cover:"/img/我是奶龙.jpg",weight:1},{name:"将军の小曲",artist:"恩！情！",url:"/music/阿悠悠 - 你若三冬（将军进行曲） (0_8xDJ沈乐版).mp3",cover:"/img/将军.png",weight:2},{name:"祁厅长进步の小曲",artist:"我太想进步了",url:"/music/鸳鸯戏 (哎呦小情郎你莫愁)(0.9x) - 略略略.mp3",cover:"/img/祁厅长の进步.jpg",weight:3},{name:"卡卡的小曲",artist:"忠！诚！",url:"/music/小轩仔仔-Shake And Sway (光州小曲).mp3",cover:"/img/全卡卡.jpg",weight:4}]})</script></body><style>#backTopBtn{display:none;position:fixed;bottom:30px;z-index:99;cursor:pointer;width:30px;height:30px;background-image:url(https://example.com/icons/backTop.svg)}</style><script>function initScrollTop(){let t=document.querySelector(".right-sidebar");if(!t)return;let e=document.createElement("div");e.id="backTopBtn",e.onclick=backToTop,t.appendChild(e),window.onscroll=function(){document.body.scrollTop>20||document.documentElement.scrollTop>20?e.style.display="block":e.style.display="none"}}function backToTop(){window.scrollTo({top:0,behavior:"smooth"})}initScrollTop()</script><script defer src=https://cn.vercount.one/js></script></body></html>