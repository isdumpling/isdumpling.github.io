<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="本文通过线性回归的数学公式示例，演示机器学习算法的实现过程。 包含梯度下降法的推导和Python代码实现，适合初学者理解基础原理。"><title>对于机器学习公式的例子</title>
<link rel=canonical href=https://example.com/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/><link rel=stylesheet href=/scss/style.min.2deeca980d2b0d58c1f28cdfd20b96c62f8357ef57495cd2d99f0054e39d1d6f.css><meta property='og:title' content="对于机器学习公式的例子"><meta property='og:description' content="本文通过线性回归的数学公式示例，演示机器学习算法的实现过程。 包含梯度下降法的推导和Python代码实现，适合初学者理解基础原理。"><meta property='og:url' content='https://example.com/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/'><meta property='og:site_name' content='一只饺子'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='机器学习'><meta property='article:tag' content='线性代数'><meta property='article:modified_time' content='2025-03-11T15:19:48+08:00'><meta property='og:image' content='https://example.com/post/img/3.jpg'><meta name=twitter:title content="对于机器学习公式的例子"><meta name=twitter:description content="本文通过线性回归的数学公式示例，演示机器学习算法的实现过程。 包含梯度下降法的推导和Python代码实现，适合初学者理解基础原理。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://example.com/post/img/3.jpg'><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/canvas-nest.js/2.0.4/canvas-nest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/1fc88c9ce946a5621185d27c512d6ae_hu_bc1d6b840fc37823.jpg width=300 height=328 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>一只饺子</a></h1><h2 class=site-description>奶龙也是龙</h2></div></header><ol class=menu-social><li><a href=https://github.com/isdumpling target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/about><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://example.com/en/>English</option><option value=https://example.com/ selected>中文</option><option value=https://example.com/ar/>عربي</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#得到权重的值>得到权重的值</a><ol><li><a href=#问题设定><strong>问题设定</strong></a><ol><li><a href=#输入特征><strong>输入特征</strong></a></li><li><a href=#真实输出><strong>真实输出</strong></a></li><li><a href=#模型结构><strong>模型结构</strong></a></li><li><a href=#目标><strong>目标</strong></a></li></ol></li><li><a href=#训练过程><strong>训练过程</strong></a><ol><li><a href=#前向传播计算预测值><strong>前向传播（计算预测值）</strong></a></li><li><a href=#计算损失均方误差><strong>计算损失（均方误差）</strong></a></li><li><a href=#反向传播计算梯度><strong>反向传播（计算梯度）</strong></a></li><li><a href=#更新参数梯度下降><strong>更新参数（梯度下降）</strong></a></li></ol></li><li><a href=#更新后的预测><strong>更新后的预测</strong></a></li><li><a href=#多轮迭代后的结果><strong>多轮迭代后的结果</strong></a></li><li><a href=#关键结论><strong>关键结论</strong></a></li></ol></li><li><a href=#y--b--sum_i-c_i--textsigmoidb_i--sum_j-w_ij-x_j>$y = b + \sum_i c_i , \text{sigmoid}(b_i + \sum_j w_{ij} x_j)$</a><ol><li><a href=#step-1设定参数值>step 1：设定参数值</a></li><li><a href=#step-2输出数据>step 2：输出数据</a></li><li><a href=#step-3计算隐藏层输出>step 3：计算隐藏层输出</a></li><li><a href=#step-4计算输出层结果>step 4：计算输出层结果</a></li></ol></li><li><a href=#lthetaapprox-ltheta-ltheta---thetagfrac12theta-thetaththeta-theta->$L(\theta)\approx L(\theta ^{&rsquo;})+L(\theta - \theta^{&rsquo;})g+\frac{1}{2}(\theta-\theta^{&rsquo;})^{T}H(\theta-\theta ^{&rsquo;})$</a><ol><li><ol><li></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/><img src=/post/img/3.jpg loading=lazy alt="Featured image of post 对于机器学习公式的例子"></a></div><div class=article-details><header class=article-category><a href=/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/>机器学习基础</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E5%AF%B9%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%8F%E7%9A%84%E4%BE%8B%E5%AD%90/>对于机器学习公式的例子</a></h2></div><footer class=article-time>最后修改:
<time class=article-time--updated datetime="2025-03-11 15:19:48 +0800 CST" title="2025-03-11 15:19:48 +0800 CST">2025-03-11</time><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 5 分钟</time></div></footer></div></header><section class=article-content><h2 id=得到权重的值>得到权重的值</h2><p>以下是一个 <strong>单层神经网络（感知机）</strong> 的完整示例，通过 <strong>手动模拟训练过程</strong>，展示如何从数据中学习权重。我们以 <strong>房价预测</strong> 为例，假设数据仅包含一个样本，目标是让模型学会调整权重和偏置。</p><h3 id=问题设定><strong>问题设定</strong></h3><h4 id=输入特征><strong>输入特征</strong></h4><ul><li>$ x_1 $（面积）：1（标准化后的值，如100平方米）</li><li>$ x_2 $（房龄）：1（标准化后的值，如5年）</li></ul><h4 id=真实输出><strong>真实输出</strong></h4><ul><li>$ y_{\text{true}} = 3 $（单位：万元）</li></ul><h4 id=模型结构><strong>模型结构</strong></h4><ul><li><strong>线性模型</strong>：$ y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b $</li><li><strong>初始参数</strong>（随机初始化）：<ul><li>权重：$ w_1 = 0.5 $, $ w_2 = -0.3 $</li><li>偏置：$ b = 0.2 $</li></ul></li></ul><h4 id=目标><strong>目标</strong></h4><p>通过梯度下降，调整 $ w_1, w_2, b $，使得 $ y_{\text{pred}} $ 接近真实值 3。</p><h3 id=训练过程><strong>训练过程</strong></h3><h4 id=前向传播计算预测值><strong>前向传播（计算预测值）</strong></h4><p>$$
y_{\text{pred}} = w_1 x_1 + w_2 x_2 + b = 0.5 \times 1 + (-0.3) \times 1 + 0.2 = 0.5 - 0.3 + 0.2 = 0.4
$$
此时预测值为 0.4 万元，与真实值 3 相差较大。</p><h4 id=计算损失均方误差><strong>计算损失（均方误差）</strong></h4><p>$$
\text{Loss} = (y_{\text{true}} - y_{\text{pred}})^2 = (3 - 0.4)^2 = 6.76
$$</p><h4 id=反向传播计算梯度><strong>反向传播（计算梯度）</strong></h4><p>对每个参数求偏导（链式法则）：</p><ul><li><strong>损失对 $ w_1 $ 的梯度</strong>：
$$
\frac{\partial \text{Loss}}{\partial w_1} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_1 = 2(0.4 - 3) \times 1 = -5.2
$$</li><li><strong>损失对 $ w_2 $ 的梯度</strong>：
$$
\frac{\partial \text{Loss}}{\partial w_2} = 2(y_{\text{pred}} - y_{\text{true}}) \cdot x_2 = 2(0.4 - 3) \times 1 = -5.2
$$</li><li><strong>损失对 $ b $ 的梯度</strong>：
$$
\frac{\partial \text{Loss}}{\partial b} = 2(y_{\text{pred}} - y_{\text{true}}) = 2(0.4 - 3) = -5.2
$$</li></ul><h4 id=更新参数梯度下降><strong>更新参数（梯度下降）</strong></h4><p>设定学习率 $ \eta = 0.1 $，更新规则：
$$
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial \text{Loss}}{\partial w}
$$</p><ul><li><strong>更新 $ w_1 $</strong>：
$$
w_1 = 0.5 - 0.1 \times (-5.2) = 0.5 + 0.52 = 1.02
$$</li><li><strong>更新 $ w_2 $</strong>：
$$
w_2 = -0.3 - 0.1 \times (-5.2) = -0.3 + 0.52 = 0.22
$$</li><li><strong>更新 $ b $</strong>：
$$
b = 0.2 - 0.1 \times (-5.2) = 0.2 + 0.52 = 0.72
$$</li></ul><h3 id=更新后的预测><strong>更新后的预测</strong></h3><p>使用新参数重新计算预测值：
$$
y_{\text{pred}} = 1.02 \times 1 + 0.22 \times 1 + 0.72 = 1.02 + 0.22 + 0.72 = 1.96
$$
损失更新为：
$$
\text{Loss} = (3 - 1.96)^2 = 1.08
$$
<strong>仅一次迭代，损失从 6.76 下降到 1.08</strong>，说明权重调整有效。</p><h3 id=多轮迭代后的结果><strong>多轮迭代后的结果</strong></h3><p>重复上述过程（假设学习率不变）：</p><div class=table-wrapper><table><thead><tr><th>迭代次数</th><th>$ w_1 $</th><th>$ w_2 $</th><th>$ b $</th><th>$ y_{\text{pred}} $</th><th>Loss</th></tr></thead><tbody><tr><td>0</td><td>0.5</td><td>-0.3</td><td>0.2</td><td>0.4</td><td>6.76</td></tr><tr><td>1</td><td>1.02</td><td>0.22</td><td>0.72</td><td>1.96</td><td>1.08</td></tr><tr><td>2</td><td>1.45</td><td>0.65</td><td>1.17</td><td>2.60</td><td>0.16</td></tr><tr><td>3</td><td>1.68</td><td>0.89</td><td>1.43</td><td>2.96</td><td>0.0016</td></tr></tbody></table></div><p>经过3次迭代，预测值 $ y_{\text{pred}} = 2.96 $ 接近真实值3，损失降至0.0016。</p><h3 id=关键结论><strong>关键结论</strong></h3><ul><li><strong>权重的本质</strong>：模型通过梯度下降，沿着损失减小的方向调整权重，逐步逼近真实值。</li><li><strong>学习率的作用</strong>：学习率 $ \eta $ 控制参数更新步幅（过大可能导致震荡，过小收敛慢）。</li><li><strong>实际训练</strong>：真实场景中需使用大量数据分批训练，而非单个样本。</li></ul><p><strong>附：Python代码模拟</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 初始参数</span>
</span></span><span class=line><span class=cl><span class=n>w1</span><span class=p>,</span> <span class=n>w2</span><span class=p>,</span> <span class=n>b</span> <span class=o>=</span> <span class=mf>0.5</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.3</span><span class=p>,</span> <span class=mf>0.2</span>
</span></span><span class=line><span class=cl><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>,</span> <span class=n>y_true</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=n>eta</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>w1</span><span class=o>*</span><span class=n>x1</span> <span class=o>+</span> <span class=n>w2</span><span class=o>*</span><span class=n>x2</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_true</span> <span class=o>-</span> <span class=n>y_pred</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 计算梯度</span>
</span></span><span class=line><span class=cl>    <span class=n>dL_dw1</span> <span class=o>=</span> <span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=n>y_pred</span> <span class=o>-</span> <span class=n>y_true</span><span class=p>)</span><span class=o>*</span><span class=n>x1</span>
</span></span><span class=line><span class=cl>    <span class=n>dL_dw2</span> <span class=o>=</span> <span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=n>y_pred</span> <span class=o>-</span> <span class=n>y_true</span><span class=p>)</span><span class=o>*</span><span class=n>x2</span>
</span></span><span class=line><span class=cl>    <span class=n>dL_db</span> <span class=o>=</span> <span class=mi>2</span><span class=o>*</span><span class=p>(</span><span class=n>y_pred</span> <span class=o>-</span> <span class=n>y_true</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=c1># 更新参数</span>
</span></span><span class=line><span class=cl>    <span class=n>w1</span> <span class=o>-=</span> <span class=n>eta</span> <span class=o>*</span> <span class=n>dL_dw1</span>
</span></span><span class=line><span class=cl>    <span class=n>w2</span> <span class=o>-=</span> <span class=n>eta</span> <span class=o>*</span> <span class=n>dL_dw2</span>
</span></span><span class=line><span class=cl>    <span class=n>b</span> <span class=o>-=</span> <span class=n>eta</span> <span class=o>*</span> <span class=n>dL_db</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>: w1=</span><span class=si>{</span><span class=n>w1</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>, w2=</span><span class=si>{</span><span class=n>w2</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>, b=</span><span class=si>{</span><span class=n>b</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>, y_pred=</span><span class=si>{</span><span class=n>y_pred</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>, Loss=</span><span class=si>{</span><span class=n>loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=y--b--sum_i-c_i--textsigmoidb_i--sum_j-w_ij-x_j>$y = b + \sum_i c_i , \text{sigmoid}(b_i + \sum_j w_{ij} x_j)$</h2><p>假设我们要根据房屋的两个特征预测房价</p><ul><li><strong>特征1($x_1$)</strong>：面积（平方米）</li><li><strong>特征2($x_2$)</strong>：房龄（年）</li></ul><p>我们设计一个简单的神经网络，结构如下：</p><ul><li><strong>输入层：</strong> 两个特征（$x_1,x_2$）</li><li><strong>隐藏层</strong>： 2个神经元（$i=1,2$）</li><li><strong>输出层</strong>： 1个输出（房价$y$）</li></ul><h3 id=step-1设定参数值>step 1：设定参数值</h3><p>假设模型已经训练完成，参数如下：</p><p><strong>隐藏层参数</strong></p><div class=table-wrapper><table><thead><tr><th>神经元</th><th>权重$w_{i1}$（面积权重）</th><th>权重$w_{i2}$(房龄权重)</th><th>偏置$b_i$</th></tr></thead><tbody><tr><td>1$(i=1)$</td><td>0.8</td><td>-0.2</td><td>0.5</td></tr><tr><td>2$(i=2)$</td><td>0.5</td><td>-0.6</td><td>-0.3</td></tr></tbody></table></div><p><strong>输出层参数</strong></p><div class=table-wrapper><table><thead><tr><th>权重$c_i$</th><th>偏置$b$</th></tr></thead><tbody><tr><td>$c_1=10$</td><td>$b=5$</td></tr><tr><td>$c_2=-8$</td><td></td></tr></tbody></table></div><h3 id=step-2输出数据>step 2：输出数据</h3><p>假设有一套房子的特征值为：</p><ul><li>面积$x_1=100m^2$</li><li>房龄$x_2=5年$</li></ul><h3 id=step-3计算隐藏层输出>step 3：计算隐藏层输出</h3><p>对每个隐藏层神经元，计算$z_i,=,b_i,+,w_{i1}x_1,+,w_{i2}x_2$，然后通过$sigmoid$激活函数得到$a_i=sigmoid(z_i)$</p><p><strong>神经元1($i=1$)的计算</strong></p><p>$$
\begin{aligned}
z_1 = b_1 + w_{11}x_1 + w_{12}x_2 = 0.5 + 0.8 \times 100 + (-0.2) \times 5 = 0.5 + 80 - 1 = 79.5\
a_1 = \text{sigmoid}(79.5) = \frac{1}{1 + e^{-79.5}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$</p><p><strong>神经元2($i=2$)的计算</strong></p><p>$$
\begin{aligned}
z_2 = b_2 + w_{21}x_1 + w_{22}x_2 = -0.3 + 0.5 \times 100 + (-0.6) \times 5 = -0.3 + 50 - 3 = 46.7\
a_2 = \text{sigmoid}(46.7) = \frac{1}{1 + e^{-46.7}} \approx 1.0 \quad (\text{几乎完全激活})
\end{aligned}
$$</p><h3 id=step-4计算输出层结果>step 4：计算输出层结果</h3><p>$$
y = b + c_1 a_1 + c_2 a_2 = 5 + 10 \times 1.0 + (-8) \times 1.0 = 5 + 10 - 8 = 7
$$</p><h2 id=lthetaapprox-ltheta-ltheta---thetagfrac12theta-thetaththeta-theta->$L(\theta)\approx L(\theta ^{&rsquo;})+L(\theta - \theta^{&rsquo;})g+\frac{1}{2}(\theta-\theta^{&rsquo;})^{T}H(\theta-\theta ^{&rsquo;})$</h2><p>由于<del>线性代数学艺不精</del>热爱线性代数，重新推导这个公式</p><ol><li><strong>回忆一维泰勒展开</strong></li></ol><p>例如，在$x&rsquo;$附件展开$f(x)$
$$
f(x)\approx f(x&rsquo;)+f&rsquo;(x&rsquo;)(x-x&rsquo;)+\frac{1}{2}f"(x&rsquo;)(x-x&rsquo;)^2
$$</p><p>对于泰勒展开公式：
$$
f(x_0,x)=\sum_{i=0}^n \frac{f^{(i)}(x_0)}{i!}(x-x_0)^i
$$</p><ol start=2><li><strong>扩展到多维情况（参数$\theta$是向量）</strong></li></ol><p>$L(\theta)\approx L(\theta ^{&rsquo;})+L(\theta - \theta^{&rsquo;})g+\frac{1}{2}(\theta-\theta^{&rsquo;})^{T}H(\theta-\theta ^{&rsquo;})$</p><p>在多维情况下，参数是向量$\theta = [\theta_1,\theta_2,&mldr;\theta_n]^T$，梯度$g$是一阶导数的推广</p><p>对于<strong>Hessian</strong>矩阵，是多元函数的二阶偏导数构成的矩阵
$$
H = \nabla^2 f = \begin{bmatrix}
\frac{\partial^2 f}{\partial \theta_1^2} & \frac{\partial^2 f}{\partial \theta_1 \partial \theta_2} & \cdots & \frac{\partial^2 f}{\partial \theta_1 \partial \theta_n} \
\frac{\partial^2 f}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 f}{\partial \theta_2^2} & \cdots & \frac{\partial^2 f}{\partial \theta_2 \partial \theta_n} \
\vdots & \vdots & \ddots & \vdots \
\frac{\partial^2 f}{\partial \theta_n \partial \theta_1} & \frac{\partial^2 f}{\partial \theta_n \partial \theta_2} & \cdots & \frac{\partial^2 f}{\partial \theta_n^2}
\end{bmatrix}
$$</p><p><strong>为什么需要转置 $(\theta - \theta&rsquo;)^\top$？</strong></p><ul><li><strong>维度匹配</strong>：假设 $\theta$ 是 $n \times 1$ 向量，梯度 $g$ 也是 $n \times 1$，Hessian H$ $H$是 $n \times n$。<ul><li>一阶项：$(\theta - \theta &lsquo;)^Tg$是$n \times 1$向量，梯度$g$也是$n\times 1$，Hessian $H$是$n \times n$（标量）<ul><li>一阶项：$(\theta - \theta &lsquo;)g$是$1\times n*n\times n * n \times 1=1\times 1$（标量）</li></ul></li></ul></li><li><strong>数学必要性</strong>：转置确保矩阵乘法维度相容。</li></ul><ol start=3><li><strong>一个具体的例子</strong></li></ol><h5 id=1-定义函数><strong>1. 定义函数</strong></h5><p>设损失函数 $L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2$，参考点 $\theta&rsquo; = [0, 0]^\top$。</p><h5 id=2-计算梯度-g><strong>2. 计算梯度 $g$</strong></h5><p>$$
g = \nabla L(\theta&rsquo;) = \begin{bmatrix} 2\theta_1 + \theta_2 \ 4\theta_2 + \theta_1 \end{bmatrix} \bigg|_{\theta&rsquo;=[0,0]} = \begin{bmatrix} 0 \ 0 \end{bmatrix}
$$</p><h5 id=3-计算-hessian-矩阵-h><strong>3. 计算 Hessian 矩阵 $H$</strong></h5><p>$$
H = \nabla^2 L(\theta&rsquo;) = \begin{bmatrix}
\frac{\partial^2 L}{\partial \theta_1^2} & \frac{\partial^2 L}{\partial \theta_1 \partial \theta_2} \
\frac{\partial^2 L}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 L}{\partial \theta_2^2}
\end{bmatrix} = \begin{bmatrix} 2 & 1 \ 1 & 4 \end{bmatrix}
$$</p><h5 id=4-泰勒展开公式><strong>4. 泰勒展开公式</strong></h5><p>在 $\theta&rsquo; = [0, 0]^\top$ 处展开：
$$
L(\theta) \approx \underbrace{0}<em>{L(\theta&rsquo;)} + \underbrace{(\theta - 0)^\top \begin{bmatrix} 0 \ 0 \end{bmatrix}}</em>{\text{一阶项}} + \frac{1}{2}(\theta - 0)^\top \begin{bmatrix} 2 & 1 \ 1 & 4 \end{bmatrix} (\theta - 0)
$$</p><p>化简后：
$$
L(\theta) \approx \frac{1}{2}\theta^\top \begin{bmatrix} 2 & 1 \ 1 & 4 \end{bmatrix} \theta = \frac{1}{2}(2\theta_1^2 + 2\theta_1\theta_2 + 4\theta_2^2)
$$</p><p>展开后与原函数一致：
$$
L(\theta) = \theta_1^2 + 2\theta_2^2 + \theta_1\theta_2
$$</p></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
<a href=/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/>线性代数</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 2025-03-11 15:19 CST</span></section><section class=article-pageviews><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/></svg>
<span>页面浏览量<span id=busuanzi_value_page_pv>Loading</span></span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85%E7%AC%94%E8%AE%B0-4%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC%E4%B8%8E%E9%9E%8D%E7%82%B9/><div class=article-image><img src=/post/img/7.jpg loading=lazy data-key data-hash=/post/img/7.jpg></div><div class=article-details><h2 class=article-title>机器学习（李宏毅）笔记 4：局部最小值与鞍点</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2025 一只饺子</section><section class=powerby><div class=site-stats><span class=stat-item><i class="fas fa-users"></i> 访客数：<span id=busuanzi_value_site_uv>Loading</span>
</span><span class=stat-item><i class="fas fa-eye"></i> 访问量：<span id=busuanzi_value_site_pv>Loading</span>
</span><span class=stat-item><a href=https://eu.umami.is/share/y6kXo4CE3oYHXQ37/farb.github.io target=_blank rel=noopener><i class="fas fa-chart-line"></i> 详细统计</a></span></div><br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><style>#backTopBtn{display:none;position:fixed;bottom:30px;z-index:99;cursor:pointer;width:30px;height:30px;background-image:url(https://example.com/icons/backTop.svg)}</style><script>function initScrollTop(){let t=document.querySelector(".right-sidebar");if(!t)return;let e=document.createElement("div");e.id="backTopBtn",e.onclick=backToTop,t.appendChild(e),window.onscroll=function(){document.body.scrollTop>20||document.documentElement.scrollTop>20?e.style.display="block":e.style.display="none"}}function backToTop(){window.scrollTo({top:0,behavior:"smooth"})}initScrollTop()</script><script defer src=https://cn.vercount.one/js></script></body></html>