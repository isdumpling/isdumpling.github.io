<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="基础知识\r交叉熵(Cross-Entropy)\r交叉熵衡量的是估计的概率分布Q近似真实分布P时所需的平均信息量 $$ H(P,Q)=-\\sum_i P(i)lnQ(i) $$\n似然函数(Likelihood Function)\r表示给定模型参数$\\theta$时，观察到当前数据集$D$的概率 $$ L(\\theta;D)=P(D|\\theta) $$ 核心思想：最大似然估计(MLE)：通过调整参数$\\theta$，使当前数据出现的概率最大化\n"><title>Pytorch实践（刘二大人）5：逻辑斯蒂回归</title>
<link rel=canonical href=https://example.com/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA5%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/><link rel=stylesheet href=/scss/style.min.2deeca980d2b0d58c1f28cdfd20b96c62f8357ef57495cd2d99f0054e39d1d6f.css><meta property='og:title' content="Pytorch实践（刘二大人）5：逻辑斯蒂回归"><meta property='og:description' content="基础知识\r交叉熵(Cross-Entropy)\r交叉熵衡量的是估计的概率分布Q近似真实分布P时所需的平均信息量 $$ H(P,Q)=-\\sum_i P(i)lnQ(i) $$\n似然函数(Likelihood Function)\r表示给定模型参数$\\theta$时，观察到当前数据集$D$的概率 $$ L(\\theta;D)=P(D|\\theta) $$ 核心思想：最大似然估计(MLE)：通过调整参数$\\theta$，使当前数据出现的概率最大化\n"><meta property='og:url' content='https://example.com/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA5%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/'><meta property='og:site_name' content='一只饺子'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='机器学习'><meta property='article:tag' content='代码实践'><meta property='article:modified_time' content='2025-03-11T15:19:48+08:00'><meta property='og:image' content='https://example.com/post/img/17.jpg'><meta name=twitter:title content="Pytorch实践（刘二大人）5：逻辑斯蒂回归"><meta name=twitter:description content="基础知识\r交叉熵(Cross-Entropy)\r交叉熵衡量的是估计的概率分布Q近似真实分布P时所需的平均信息量 $$ H(P,Q)=-\\sum_i P(i)lnQ(i) $$\n似然函数(Likelihood Function)\r表示给定模型参数$\\theta$时，观察到当前数据集$D$的概率 $$ L(\\theta;D)=P(D|\\theta) $$ 核心思想：最大似然估计(MLE)：通过调整参数$\\theta$，使当前数据出现的概率最大化\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://example.com/post/img/17.jpg'><link rel="shortcut icon" href=/img/icon.svg><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/canvas-nest.js/2.0.4/canvas-nest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/1fc88c9ce946a5621185d27c512d6ae_hu_bc1d6b840fc37823.jpg width=300 height=328 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>一只饺子</a></h1><h2 class=site-description>奶龙也是龙</h2></div></header><ol class=menu-social><li><a href=https://github.com/isdumpling target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/about><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://example.com/en/>English</option><option value=https://example.com/ selected>中文</option><option value=https://example.com/ar/>عربي</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#基础知识>基础知识</a><ol><li><a href=#交叉熵cross-entropy>交叉熵(Cross-Entropy)</a></li><li><a href=#似然函数likelihood-function>似然函数(Likelihood Function)</a></li><li><a href=#对数似然log-likelihood>对数似然(Log-Likelihood)</a></li><li><a href=#损失函数loss-function>损失函数(Loss Function)</a></li></ol></li><li><a href=#数学原理>数学原理</a><ol><li><a href=#模型结构线性组合sigmoid函数>模型结构：线性组合+Sigmoid函数</a></li><li><a href=#损失函数交叉熵>损失函数：交叉熵</a></li><li><a href=#参数优化梯度下降>参数优化：梯度下降</a></li><li><a href=#决策边界>决策边界</a></li></ol></li><li><a href=#思路>思路</a><ol><li><a href=#数据准备>数据准备</a></li><li><a href=#模型定义>模型定义</a></li><li><a href=#损失函数与优化器>损失函数与优化器</a></li><li><a href=#训练循环>训练循环</a></li></ol></li><li><a href=#代码实现>代码实现</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA5%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/><img src=/post/img/17.jpg loading=lazy alt="Featured image of post Pytorch实践（刘二大人）5：逻辑斯蒂回归"></a></div><div class=article-details><header class=article-category><a href=/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA5%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/>Pytorch实践（刘二大人）5：逻辑斯蒂回归</a></h2></div><footer class=article-time>最后修改:
<time class=article-time--updated datetime="2025-03-11 15:19:48 +0800 CST" title="2025-03-11 15:19:48 +0800 CST">2025-03-11</time><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 3 分钟</time></div></footer></div></header><section class=article-content><h2 id=基础知识>基础知识</h2><h3 id=交叉熵cross-entropy>交叉熵(Cross-Entropy)</h3><p>交叉熵衡量的是估计的概率分布Q近似真实分布P时所需的平均信息量
$$
H(P,Q)=-\sum_i P(i)lnQ(i)
$$</p><h3 id=似然函数likelihood-function>似然函数(Likelihood Function)</h3><p>表示给定模型参数$\theta$时，观察到当前数据集$D$的概率
$$
L(\theta;D)=P(D|\theta)
$$
<strong>核心思想</strong>：最大似然估计(MLE)：通过调整参数$\theta$，使当前数据出现的概率最大化</p><h3 id=对数似然log-likelihood>对数似然(Log-Likelihood)</h3><p>连乘容易导致数值下溢或溢出，取对数将乘法转为加法
$$
lnL(\theta;D)=\sum_{i=1}^N lnP(y_i|x_i;\theta)
$$</p><h3 id=损失函数loss-function>损失函数(Loss Function)</h3><p>在最大似然估计中，负对数似然常被用作损失函数
$$
\mathcal{L}(w,b)=-\sum_{i=1}^N[y_iln\hat{y_i}+(1-y_i)ln(1-\hat{y})]
$$</p><h2 id=数学原理>数学原理</h2><h3 id=模型结构线性组合sigmoid函数>模型结构：线性组合+Sigmoid函数</h3><p>逻辑斯蒂回归的核心是将线性回归的输出映射到概率空间（0和1之间）</p><ul><li><strong>线性部分</strong>：<ul><li>对于输入特征向量$\vec{x}=[x_1,x_2,&mldr;,x_n]$，计算线性组合：$z=\vec{w}^T\vec{x}+b=w_1x_1+w_2x_2+&mldr;+w_nx_n+b$</li><li>其中，$\vec{w}$是权重向量，$b$是偏置项</li></ul></li><li><strong>Sigmoid函数</strong>：<ul><li>将线性输出$z$通过Sigmoid函数转换为概率：$P(y=1|\vec{x})=\sigma(z)=\frac{1}{1+e^{-z}}$</li></ul></li></ul><h3 id=损失函数交叉熵>损失函数：交叉熵</h3><p>逻辑斯蒂回归通过<strong>极大似然估计</strong>（MLE）求解参数，对应的损失函数是<strong>交叉熵损失</strong></p><ul><li><strong>似然函数</strong>：对每个样本$(x_i,y_i)$，其似然为$P(y_i|x_i)=\sigma(z_i)^{y_i}\cdot (1-\sigma(z_i))^{1-y_i}$</li><li><strong>对数似然与损失似然</strong>：$\mathcal{L}(w,b)=-\sum_{i=1}^N [y_i ln\sigma(z_i)+(1-y_i)ln(1-\sigma(z_i))]$</li></ul><h3 id=参数优化梯度下降>参数优化：梯度下降</h3><p>通过梯度下降法迭代更新权重$w$和偏置$b$</p><ul><li><strong>梯度计算</strong>：Sigmoid函数的导数$\sigma &lsquo;(z)=\sigma(z)(1-\sigma(z))$，损失函数$w$和$b$的梯度为：</li></ul><p>$$
\frac{\partial \mathcal{L}}{\partial w_j}=\sum_{i=1}^{N}(\sigma(z_i)-y_i)x_{ij},\frac{\partial\mathcal{L}}{\partial b}=\sum_{i=1}{N}(\sigma(z_i)-y_i)
$$</p><ul><li><strong>参数更新</strong>:</li></ul><p>$$
w=w-\eta \frac{\partial \mathcal{L}}{\partial w}, b=b-\eta\frac{\partial \mathcal{L}}{\partial b}
$$</p><h3 id=决策边界>决策边界</h3><p>逻辑斯蒂回归的决策边界是线性的，由方程$w^Tx+b=0$定义</p><ul><li>当$\sigma(z)\ge 0.5$，预测$y=1$（即$z\ge 0$）；</li><li>否则预测$y=0$</li></ul><h2 id=思路>思路</h2><h3 id=数据准备>数据准备</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.0</span><span class=p>],[</span><span class=mf>2.0</span><span class=p>],[</span><span class=mf>3.0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>y_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>0</span><span class=p>],[</span><span class=mi>0</span><span class=p>],[</span><span class=mi>1</span><span class=p>]])</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>输入数据</strong> <code>x_data</code>：3个样本，每个样本1个特征（形状为 <code>[3, 1]</code>）。</li><li><strong>标签数据</strong> <code>y_data</code>：对应的二分类标签（0或1）。<ul><li>当特征值为1.0和2.0时，标签是0；特征值为3.0时，标签是1。</li><li>这可以理解为模型需要学习“当特征值大于某个阈值时预测为1”。</li></ul></li></ul><h3 id=模型定义>模型定义</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LogisticRegressionModel</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>LogisticRegressionModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=n>__init</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>)</span> <span class=c1># 表示输入和输出维度均为1</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y_pred</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y_pred</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=损失函数与优化器>损失函数与优化器</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>BCELoss</span><span class=p>(</span><span class=n>size_average</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>  <span class=c1># 二元交叉熵损失（累加模式）</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>  <span class=c1># 随机梯度下降</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><strong>损失函数</strong> <code>BCELoss</code>：二元交叉熵损失（Binary Cross Entropy Loss），用于衡量预测概率与真实标签的差异。<ul><li><code>size_average=False</code> 表示损失是<strong>累加</strong>而非平均（PyTorch新版本中已更名为 <code>reduction='sum'</code>）。</li></ul></li><li><strong>优化器</strong> <code>SGD</code>：随机梯度下降，学习率 <code>lr=0.01</code>。</li></ul><h3 id=训练循环>训练循环</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x_data</span><span class=p>)</span>          <span class=c1># 前向传播</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>y_data</span><span class=p>)</span>  <span class=c1># 计算损失</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>           <span class=c1># 清空梯度</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>                 <span class=c1># 反向传播计算梯度</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>                <span class=c1># 更新参数（w和b）</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=代码实现>代码实现</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>1.0</span><span class=p>],</span> <span class=p>[</span><span class=mf>2.0</span><span class=p>],</span> <span class=p>[</span><span class=mf>3.0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>y_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>2.0</span><span class=p>],</span> <span class=p>[</span><span class=mf>4.0</span><span class=p>],</span> <span class=p>[</span><span class=mf>6.0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LinearModel</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>LinearModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y_pred</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y_pred</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>LinearModel</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>MSELoss</span><span class=p>(</span><span class=n>reduction</span> <span class=o>=</span> <span class=s1>&#39;sum&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span> <span class=o>=</span> <span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>y_data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>epoch</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;w = &#39;</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;b = &#39;</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>linear</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x_test</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>4.0</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>y_test</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y_pred = &#39;</span><span class=p>,</span> <span class=n>y_test</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
<a href=/tags/%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/>代码实践</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 2025-03-11 15:19 CST</span></section><section class=article-pageviews><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/></svg>
<span>页面浏览量<span id=busuanzi_value_page_pv>Loading</span></span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/pytorch%E8%AF%AD%E6%B3%95/><div class=article-image><img src=/post/img/12.jpg loading=lazy data-key data-hash=/post/img/12.jpg></div><div class=article-details><h2 class=article-title>PyTorch语法</h2></div></a></article><article class=has-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA1%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/><div class=article-image><img src=/post/img/13.jpg loading=lazy data-key data-hash=/post/img/13.jpg></div><div class=article-details><h2 class=article-title>Pytorch实践（刘二大人）1：线性模型</h2></div></a></article><article class=has-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA2%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/><div class=article-image><img src=/post/img/14.jpg loading=lazy data-key data-hash=/post/img/14.jpg></div><div class=article-details><h2 class=article-title>Pytorch实践（刘二大人）2：梯度下降算法</h2></div></a></article><article class=has-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/><div class=article-image><img src=/post/img/15.jpg loading=lazy data-key data-hash=/post/img/15.jpg></div><div class=article-details><h2 class=article-title>Pytorch实践（刘二大人）3：反向传播</h2></div></a></article><article class=has-image><a href=/p/pytorch%E5%AE%9E%E8%B7%B5%E5%88%98%E4%BA%8C%E5%A4%A7%E4%BA%BA4%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/><div class=article-image><img src=/post/img/16.jpg loading=lazy data-key data-hash=/post/img/16.jpg></div><div class=article-details><h2 class=article-title>Pytorch实践（刘二大人）4：用PyTorch实现线性回归</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2025 一只饺子</section><section class=powerby><div class=site-stats><span class=stat-item><i class="fas fa-users"></i> 访客数：<span id=busuanzi_value_site_uv>Loading</span>
</span><span class=stat-item><i class="fas fa-eye"></i> 访问量：<span id=busuanzi_value_site_pv>Loading</span>
</span><span class=stat-item><a href=https://eu.umami.is/share/y6kXo4CE3oYHXQ37/farb.github.io target=_blank rel=noopener><i class="fas fa-chart-line"></i> 详细统计</a></span></div><br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><style>#backTopBtn{display:none;position:fixed;bottom:30px;z-index:99;cursor:pointer;width:30px;height:30px;background-image:url(https://example.com/icons/backTop.svg)}</style><script>function initScrollTop(){let t=document.querySelector(".right-sidebar");if(!t)return;let e=document.createElement("div");e.id="backTopBtn",e.onclick=backToTop,t.appendChild(e),window.onscroll=function(){document.body.scrollTop>20||document.documentElement.scrollTop>20?e.style.display="block":e.style.display="none"}}function backToTop(){window.scrollTo({top:0,behavior:"smooth"})}initScrollTop()</script><script defer src=https://cn.vercount.one/js></script></body></html>